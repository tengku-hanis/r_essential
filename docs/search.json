[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Essentials: A Beginner’s Guide to Data Analysis",
    "section": "",
    "text": "Welcome\n\n\n\n\n\n\nThis book is in its early drafting stages; several chapters of the book are unfinished and all of it will go through significant revision and refinement.\n\n\n\n\n\n\nThis book is intended for anyone interested in using R for applied statistical analysis and machine learning. I first encountered R (specifically RStudio) during my master’s studies, and it wasn’t an easy journey—especially as someone from a non-coding background. Initially, I was introduced to software like SPSS and STATA, and eventually R during my master’s studies. R felt unfamiliar and complex by comparison to the previous software. However, those who could use R proficiently seemed impressive to me, which motivated me to keep going. Surprisingly, the learning process became much easier as I keep using R.\nLooking back, I wish I had known certain things earlier or that experienced R users had shared their insights from the start. This book is my attempt to provide those insights, covering the key concepts and tips I wish I had had when I started. I hope it sparks interest in R for others and helps them fully utilize its capabilities.\nFinally, I just want to say thanks to my amazing wife, Asmaq, my parents, Tengku Mokhtar and Nor Malaysia, and my in-laws, Mazalan and Salmeh, for all their understanding. Writing this book means taking the time I would usually spend with them, and while they might not fully get my obsession with R and data analysis (especially my wife!), they’ve supported me every step of the way.\nTengku Muhammad Hanis Bin Tengku Mokhtar, PhD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the book",
    "section": "",
    "text": "This book is in its early drafting stages; many portions of the book are unfinished and all of it will go through significant revision and refinement.\n\n\n\n\n\n\nThis book is designed for beginner and novice R users, with chapters structured sequentially to introduce R step-by-step, starting with foundational topics and progressing to more complex material.\nChapters 1 to 3 introduce readers to R and RStudio. For those already somewhat familiar with R, these chapters may seem straightforward and can be skipped without issue.\nChapters 4 and 5 provide essential knowledge on basic R coding, commonly used in data analysis projects. It is advised to read these chapters thoroughly before moving on, as they lay the groundwork for more advanced topics.\nChapter 6 focuses on basic visualizations and plotting techniques. Readers will learn how to use base R functions for visual representation and will be introduced to ggplot2, a highly regarded package for data visualisation in R.\nChapter 7 covers loop, apply family, and function, topics that may pose a challenge for beginners. This chapter aims to equip readers with the skills needed for more efficient R coding. While not critical for initial learning, understanding these concepts will become increasingly important as one progresses in data analysis.\nChapters 8 and 9 delve into essential skills for data exploration and descriptive statistics. Mastering these will enable readers to gain deeper insights into their data and prepare them for more advanced analysis techniques.\nChapter 10 concludes the book by summarizing previous content and offering guidance on next steps to further enhance R and data analysis skills.\nEach chapter kicks off with a quote—I hope you enjoy it! To wrap things up, each chapter ends with chapter summary and revision questions.\nHappy learning!",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "what-is-r.html",
    "href": "what-is-r.html",
    "title": "1  What is R?",
    "section": "",
    "text": "1.1 R\nR is a language and environment for statistical computing and graphics.\nThat basically summarises this whole chapter.\nReaders who are not interested in knowing more about R and its history can skip this chapter and move on to more practical chapters.\nWell, for those who keep reading this, I guess you are interested to know more about the story of how R came to be. Do not worry, this chapter is only going to cover a short version of the history of R, so, that we can appreciate this software.\nR was a successor of S language. It was developed by Ross Ihaka and Robert Gentleman at the University of Auckland in 1991. R was made known to the public only in 1993. The R version 1.0.0 was released in 2000.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#r",
    "href": "what-is-r.html#r",
    "title": "1  What is R?",
    "section": "",
    "text": "Figure 1.1: The logo of R software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#rstudio-and-posit",
    "href": "what-is-r.html#rstudio-and-posit",
    "title": "1  What is R?",
    "section": "1.2 RStudio and Posit",
    "text": "1.2 RStudio and Posit\n\n\n\n\n\n\nFigure 1.2: The logo of RStudio.\n\n\n\nIn 2009, a company known as RStudio, Inc. was founded by Joseph J. Allaire, which later developed the RStudio software. RStudio software is an integrated development environment (IDE) which helps make R more user-friendly, especially for those without a programming background. RStudio IDE is unequivocally the most commonly used IDE for R software. The company, RStudio, Inc. later changed its corporation to a public benefit corporation (PBC) in 2020, thus, known as RStudio, PBC. Subsequently, in 2022, the company changed its name to Posit Software, PBC to cater to a larger demography of the data science community.\n\n\n\n\n\n\nFigure 1.3: The logo of Posit Software, PBC.\n\n\n\nDespite the changes in Posit company, they still strongly support the development and maintenance of RStudio IDE specifically and R in general.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#other-ides",
    "href": "what-is-r.html#other-ides",
    "title": "1  What is R?",
    "section": "1.3 Other IDEs",
    "text": "1.3 Other IDEs\nAs you may have guessed, we going to use RStudio in this book. However, there are other IDEs available. A few that are more common are:\n\nJupyter Notebook\nJupyterLab\nVisual Studio\n\nGiven that RStudio is initially developed for R (currently we can use Python as well in RStudio), a lot of functionalities work seamlessly with R. Thus, make it easier for R beginners and novices to use it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#clouds",
    "href": "what-is-r.html#clouds",
    "title": "1  What is R?",
    "section": "1.4 Clouds",
    "text": "1.4 Clouds\nThere are a few options to use R in a cloud. Meaning that we do not need to install anything on our machines.\n\nPosit Cloud\nGoogle Colab\nKaggle\n\nThe first two clouds are free with limited use, though you need to make an account. Kaggle is totally free to use as far as I know. However, Kaggle does not have functions such as code completion which is very helpful to beginners. This function is available in the first two clouds.\nHowever, if you are looking for something more familiar to RStudio, Posit Cloud is the best choice. The functionalities and the overall look of the Posit Cloud are identical to RStudio.\n\n\n\n\n\n\nFigure 1.4: The interface of RStudio in Posit Cloud.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#chapter-summary",
    "href": "what-is-r.html#chapter-summary",
    "title": "1  What is R?",
    "section": "1.5 Chapter summary",
    "text": "1.5 Chapter summary\nIn this chapter, we learn about what are R and RStudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "what-is-r.html#revision",
    "href": "what-is-r.html#revision",
    "title": "1  What is R?",
    "section": "1.6 Revision",
    "text": "1.6 Revision\n\nWhat is the difference between R and RStudio IDE?\nWhat is the difference between RStudio IDE, RStudio, Inc., and RStudio, PBC?\nWhat are other IDEs for R?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "2  Installing R and RStudio",
    "section": "",
    "text": "2.1 Installing R\nR can be installed regardless of your operating system. R is available for Windows, Mac, and Linux users. This page (https://cran.r-project.org/) contains all the necessary information needed to install R. Additionally, if you are stuck, I highly recommend you to watch a few YouTube videos on how to install R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "install-r.html#installing-rstudio",
    "href": "install-r.html#installing-rstudio",
    "title": "2  Installing R and RStudio",
    "section": "2.2 Installing RStudio",
    "text": "2.2 Installing RStudio\nOnce you have installed R, you need to install RStudio IDE. This page (https://posit.co/download/rstudio-desktop/) contains all the information including the related links necessary to download and install RStudio. Again, if you are stuck, I highly recommend you watch a few YouTube videos on how to install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "install-r.html#installing-rtools",
    "href": "install-r.html#installing-rtools",
    "title": "2  Installing R and RStudio",
    "section": "2.3 Installing RTools",
    "text": "2.3 Installing RTools\nThis process is specific to Windows users only. If you are a Linux or Mac user, feel free to skip this part. Rtools is a collection of tools required to build R packages from source on Windows systems. Basically, you need RTools to install the unofficial packages from GitHub, GitLab or other repositories. We going to cover what are R packages in the Section 4.6. For now, just know that to fully utilise the capabilities of R, you need to have RTools in your machine.\nThis page (https://cran.r-project.org/bin/windows/Rtools/rtools40.html) contains all the necessary information on how to install RTools on your machine. The basic steps as outlined on the page are:\n\nDownload RTools\nInstall the RTools\nPut RTools on the PATH",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "install-r.html#other-alternatives",
    "href": "install-r.html#other-alternatives",
    "title": "2  Installing R and RStudio",
    "section": "2.4 Other alternatives",
    "text": "2.4 Other alternatives\nOn the rare occasion that you are unable to install R or RStudio, you always have the option to use the Posit Cloud. As long as you have a Google account, you should be able to use the Posit Cloud freely. The free account is limited, however, it is more than enough for you to use throughout this book. Additionally, you can use the Google Colab, though, the user interface is slightly different to RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "install-r.html#chapter-summary",
    "href": "install-r.html#chapter-summary",
    "title": "2  Installing R and RStudio",
    "section": "2.5 Chapter summary",
    "text": "2.5 Chapter summary\nIn this chapter, we learn how to install R and RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "install-r.html#revision",
    "href": "install-r.html#revision",
    "title": "2  Installing R and RStudio",
    "section": "2.6 Revision",
    "text": "2.6 Revision\n\nWhat is RTools?\nWhat are the options if you can not install R and RStudio?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html",
    "href": "use-rstudio.html",
    "title": "3  Introduction to RStudio",
    "section": "",
    "text": "3.1 Panes in RStudio\nThe first time we open RStudio we will see three panes as in Figure 3.1.\nPane 1 consists of three tabs:\nNext, Pane 2 consists of four tabs:\nLastly, Pane 3 consists of five tabs:\nThe information can be quite overwhelming, especially if you are new to R and RStudio. At this moment, you do not actually need to know every detail functionalities of each pane and tab yet. Once you become more familiar with RStudio, all this information will become second-hand to you.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#panes-in-rstudio",
    "href": "use-rstudio.html#panes-in-rstudio",
    "title": "3  Introduction to RStudio",
    "section": "",
    "text": "Figure 3.1: RStudio interface.\n\n\n\n\n\nConsole - This pane is where R codes can be typed and executed. The console also is where the output will be displayed. However, any R codes typed into the console can not be saved.\nTerminal - This is a command-line interface that allows users to interact with the system shell, much like other terminal applications on your computer. It supports tasks such as navigating the file system, running command-line tools, and managing files, which can be useful for integrating non-R tasks into your workflow. However, for beginners, this tab is relatively unimportant.\nBackground Jobs - This pane enables users to run R scripts in the background without interrupting ongoing work in the Console. This feature is helps run long or complex tasks, as you can continue coding or working in the RStudio environment while the script executes independently.\n\n\n\nEnvironment - This pane displays all the active objects in the current R session, such as data frames, variables, functions, and vectors.\nHistory - This pane keeps a record of all commands that have been executed in the R console during the session\nConnections - This pane facilitates managing database connections within RStudio. It allows users to connect to external databases, view database contents, and run SQL queries directly from the IDE.\nTutorial - The Tutorial pane is part of RStudio’s learnr package, which hosts interactive tutorials directly within the IDE.\n\n\n\nFiles - This pane allows users to navigate, create, delete, and manage files within the current working directory. It helps in organizing project files, accessing scripts, data files, and outputs.\nPlots - This pane displays graphical output generated by R code, such as charts, graphs, and plots.\nPackages - This pane shows a list of installed R packages and their status (loaded or not). It also allows users to install, remove, or update packages, providing an easy way to manage package dependencies for projects.\nHelp - This pane provides access to documentation for R functions, packages, and commands.\nViewer - The Viewer pane is used for displaying web content such as interactive visualizations, markdown files, and other HTML outputs directly within RStudio.\nPresentation - This pane supports presenting R Markdown or Quarto documents in an interactive format. For example, if you are making a slide or HTML presentation, it will appear in this pane.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#working-directory",
    "href": "use-rstudio.html#working-directory",
    "title": "3  Introduction to RStudio",
    "section": "3.2 Working directory",
    "text": "3.2 Working directory\nIn Figure 3.1, we have a Files tab in Pane 3. This is usually where your working directory is located. However, we can also check using the R code below:\n\ngetwd()\n\nAdditionally, the working directory can also be changed to your preferred location.\n\nsetwd(\"C:/Users/tengk/OneDrive/Desktop\")\n\nHere, I changed my working directory to my desktop folder. A good practice when running the analysis in R is to set up your working directory before you start any analysis project. So, any outputs and figures generated during the analysis will be saved in your preferred working directory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#r-script",
    "href": "use-rstudio.html#r-script",
    "title": "3  Introduction to RStudio",
    "section": "3.3 R script",
    "text": "3.3 R script\nOne of the few things to do before running the analysis project (besides setting up the working directory) is to type the R codes in the R script. R scripts is a plain text file with the extension .R. R script can be saved and the R codes in it can be re-run if needed.\nThere are a few ways to open the R script.\n\nClick on File (upper left side of RStudio) &gt; New File &gt; R Script.\nClick on the green plus button (below the File tab) &gt; R Script as shown in Figure 3.2.\n\n\n\n\n\n\n\nFigure 3.2: Opening the R Script.\n\n\n\nOnce you manage to open the R Script, you will see an additional pane as shown in Figure 3.3.\n\n\n\n\n\n\nFigure 3.3: R Script interface.\n\n\n\nThe R Script can be saved and additionally, at the lower right side of the pane, we can see R Script, confirming that this newly open file is an R Script.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#updating-r-and-rstudio",
    "href": "use-rstudio.html#updating-r-and-rstudio",
    "title": "3  Introduction to RStudio",
    "section": "3.4 Updating R and RStudio",
    "text": "3.4 Updating R and RStudio\ninstallr package can be used to update R. You will what is an R package and how to install it in Chapter 5.\n\ninstallr::updateR()\n\nOnce you run installr::updateR() either in the Console or R Script, this package will check whether there is a newer version of R or not. If the newer version of R is available, this package installs the newer version after asking a series of questions such as do you want to transfer your old packages to the new version of R and whether you want to update the packages or not.\nTo update the RStudio, you can click the Help tab (at the top of the RStudio) &gt; Check for Updates.\n\n\n\n\n\n\nFigure 3.4: Updating RStudio",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#chapter-summary",
    "href": "use-rstudio.html#chapter-summary",
    "title": "3  Introduction to RStudio",
    "section": "3.5 Chapter summary",
    "text": "3.5 Chapter summary\nIn this chapter, we learn about:\n\nBasic interface of RStudio.\nHow to set up the working directory.\nHow to update R and RStudio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "use-rstudio.html#revision",
    "href": "use-rstudio.html#revision",
    "title": "3  Introduction to RStudio",
    "section": "3.6 Revision",
    "text": "3.6 Revision\n\nChange your working directory to your desktop.\nTry creating a new R Script, rename it as Test.R, and save it in your new working directory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to RStudio</span>"
    ]
  },
  {
    "objectID": "basic-r.html",
    "href": "basic-r.html",
    "title": "4  Basics of R",
    "section": "",
    "text": "4.1 Getting help\nProbably the most basic thing to know is how to get help in R. Besides a quick Google search or asking ChatGPT, R also provides a help function. The help function can be accessed using ?.\n?mean()\nThe code above will open the Help pane, which explains what the function mean() does.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#getting-help",
    "href": "basic-r.html#getting-help",
    "title": "4  Basics of R",
    "section": "",
    "text": "Figure 4.1: The Help pane in RStudio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#executing-the-code",
    "href": "basic-r.html#executing-the-code",
    "title": "4  Basics of R",
    "section": "4.2 Executing the code",
    "text": "4.2 Executing the code\nThe codes can be typed either in R Script or Console. The code in R Scripts can be executed by placing the cursor at any line of the codes and clicking Ctrl + Enter in Windows and Cmd + Enter in Macs.\n\n# Example 1: A single line of code\nmean(1:10) \n\n# Example 2: A multiple lines of code\nc(1:10, 10.6, 11.9) |&gt;\n  mean() |&gt;\n  round(digits = 1)\n\nIn the second example (example of multiple lines of code), a cursor can be placed at any line of code. Additionally, the codes in the Console can be executed by clicking Enter only. If you want to run any code in this book, you should copy and paste it into the R Script instead of the Console, especially if there are multiple lines of code. Notably, the codes should be run in sequence unless the current line of codes is independent from the previous line of codes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#data-types-in-r",
    "href": "basic-r.html#data-types-in-r",
    "title": "4  Basics of R",
    "section": "4.3 Data types in R",
    "text": "4.3 Data types in R\nThere are a few data types in R:\n\nNumeric\nInteger\nLogical\nCharacter\nComplex\n\nLet us see the example in R:\nFor the numeric:\n\nx1 &lt;- 11\nx2 &lt;- 11.9\n\nclass(x1); class(x2)\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\nBoth numbers are recognised as numeric in R. For integers, the number should be denoted by ‘L’ to be recognised as an integer.\n\nx3 &lt;- 11L\n\nclass(x3)\n\n[1] \"integer\"\n\n\nFor logical values, the boolean operators such as ‘FALSE’ and ‘TRUE’ are examples of logical values.\n\nx4 &lt;- c(TRUE, FALSE)\n\nclass(x4)\n\n[1] \"logical\"\n\n\nNext, we have character values.\n\nx5 &lt;- c(\"fruit\", \"apple\")\n\nclass(x5)\n\n[1] \"character\"\n\n\nLastly, we have complex values. The type of data is usually used to store numbers and imaginary components (for example, i in the code below).\n\nx6 &lt;- 9 + 3i\n\nclass(x6)\n\n[1] \"complex\"\n\n\nIt is important to note the data type for each value as the function for numeric values can only be applied to numeric values. For example, if we want to find a mean value.\n\nnumeric_val &lt;- 1:10 #list out all numbers between 1 and 10\nnumeric_val\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ncharacter_val &lt;- letters[1:10] #list out the first 10 alphabets\ncharacter_val\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n\nNow, let us try applying the mean function to both data.\n\nmean(numeric_val)\n\n[1] 5.5\n\nmean(character_val)\n\nWarning in mean.default(character_val): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nSo, R gives us a warning that the ‘character_val’ is not a numeric or logical value. Thus, the returning NA mean not available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#data-structure-in-r",
    "href": "basic-r.html#data-structure-in-r",
    "title": "4  Basics of R",
    "section": "4.4 Data structure in R",
    "text": "4.4 Data structure in R\nThere are a few data structures in R:\n\nVector\nMatrix\nArray\nData frame\nList\n\n\n\n\n\n\n\nFigure 4.2: Data structures in R.\n\n\n\nDepending on the fields, certain data structures are more common compared to others.\n\n4.4.1 Vector\nVector is the most basic data structure in R. It can contain one data type at a time.\n\nvec_data &lt;- c(1, 2, 3, 4)\nvec_data\n\n[1] 1 2 3 4\n\n\nThe structure of the data can be checked using the function str().\n\nstr(vec_data)\n\n num [1:4] 1 2 3 4\n\n\nWe can further confirm whether vec_data is a vector or not by using the is.vector() function.\n\nis.vector(vec_data)\n\n[1] TRUE\n\n\nA TRUE result indicates that the data is a vector type.\n\n\n4.4.2 Matrix\nA matrix contains at least a single row and a single column. Contrary, to a vector which contains only a single row or a single column.\n\nmat_data &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\nmat_data\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nBy using str() function, we can see that the values are numerical and we have a matrix with 3 rows and 2 columns.\n\nstr(mat_data)\n\n num [1:3, 1:2] 1 2 3 4 5 6\n\n\nNext, we can confirm that our data is a matrix by using is.matrix().\n\nis.matrix(mat_data)\n\n[1] TRUE\n\n\n\n\n4.4.3 Array\nAn array is quite similar to a matrix except that it can contain several layers of rows and columns.\n\narr_data &lt;- array(data = c(1:6, 10:16), dim = c(2, 3, 2))\narr_data\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   12   14\n[2,]   11   13   15\n\n\nAs we can see, we have an array of 2 layers with each layer having 2 rows and 3 columns. By using the str() function, we can see that R recognises the array has integer values with 3 dimensions. The first dimension, 1:2 refers to the rows, the second dimension, 1:3 refers to the columns, and the last dimension, 1:2 refers to the layers.\n\nstr(arr_data)\n\n int [1:2, 1:3, 1:2] 1 2 3 4 5 6 10 11 12 13 ...\n\n\nis.array() can be used to ensure the data structure.\n\nis.array(arr_data)\n\n[1] TRUE\n\n\n\n\n4.4.4 Data frame\nA data frame is the extension of the matrix data structure. The difference between the former and the latter, the former contains the column names and each column may contain different data types.\n\ndf_data &lt;- data.frame(\n  ID = 1:5,\n  Name = c(\"Mamat\", \"Abu\", \"Ali\", \"Chong\", \"Eva\"),\n  Age = c(25, 30, 22, 35, 28),\n  Score = c(89, 95, 76, 88, 92)\n)\ndf_data\n\n  ID  Name Age Score\n1  1 Mamat  25    89\n2  2   Abu  30    95\n3  3   Ali  22    76\n4  4 Chong  35    88\n5  5   Eva  28    92\n\n\nWe can further check the data structure and type using str(). So, our data structure is a data frame, consisting of 4 columns; the first column is an integer, the second column is a character, third and fourth columns are numeric.\n\nstr(df_data)\n\n'data.frame':   5 obs. of  4 variables:\n $ ID   : int  1 2 3 4 5\n $ Name : chr  \"Mamat\" \"Abu\" \"Ali\" \"Chong\" ...\n $ Age  : num  25 30 22 35 28\n $ Score: num  89 95 76 88 92\n\n\nWe can double-check the data structure using is.data.frame().\n\nis.data.frame(df_data)\n\n[1] TRUE\n\n\n\n\n4.4.5 List\nLastly, we have a list. So, the list is a more advanced data structure in which we can have different data structures in a data structure. Let us see the example of a list in which we combine the previous vector and matrix data structures.\n\nlist_data &lt;- list(\n  \"vector\" = vec_data,\n  \"matrix\" = mat_data\n)\nlist_data\n\n$vector\n[1] 1 2 3 4\n\n$matrix\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nWe can further assess each data in the list using the $ symbol.\n\nlist_data$vector\n\n[1] 1 2 3 4\n\n\nBy using str(), we see our data is a list with 2 elements or components; a vector and matrix.\n\nstr(list_data)\n\nList of 2\n $ vector: num [1:4] 1 2 3 4\n $ matrix: num [1:3, 1:2] 1 2 3 4 5 6\n\n\nTo further confirm our data is a list, we can use is.list().\n\nis.list(list_data)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#data-in-r",
    "href": "basic-r.html#data-in-r",
    "title": "4  Basics of R",
    "section": "4.5 Data in R",
    "text": "4.5 Data in R\nR itself contains the internal data that you can load for practice or other purposes. You can run data() in the Console to see what are the data available in R.\n\ndata()\n\nSo, for example, if you want to load any of these data, you can type the name of the datasets. You will see under the Environment pane, the chickWeight dataset is loaded to your environment.\n\ndata(\"ChickWeight\")\n\nAdditionally, it is quite common that R packages have their own datasets as well. We will see this many times in this book going forward. Lastly, R is capable of reading different data formats.\n\n\n\nTable 4.1: Common data formats and corresponding R codes for reading them.\n\n\n\n\n\nFormat\nR codes\n\n\n\n\n.csv\nread.csv()\n\n\n.sav (SPSS)\nhaven::read_sav()\n\n\n.xlsx (Excel)\nreadxl::read_excel()\n\n\n.txt\nread.table()\n\n\n.dta (STATA)\nhaven::read_dta()\n\n\n\n\n\n\nThe data formats that can be read are limited to the one listed in the table. Almost all data formats can be read in R. An efficient way to know whether R is capable of reading certain data formats by just a quick Google search.\nFor the rest of the chapter we going to the iris dataset, which is already available in R. To know more about this dataset, you can type the below code in the console.\n\n?iris",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#sec-basic-r-packages",
    "href": "basic-r.html#sec-basic-r-packages",
    "title": "4  Basics of R",
    "section": "4.6 Packages",
    "text": "4.6 Packages\nA package is a collection of functions and sample data that can be utilised for certain tasks. Certain functions in R are already loaded when you open R or RStudio. However, to use more advanced functions we need to install and load a package.\nThe packages can installed using install.packages(). For example, the code below will install the dplyr package which is commonly utilised for data wrangling and manipulation.\n\ninstall.packages(\"dplyr\")\n\nTo use the installed packages, we need to load the packages using the library() function.\n\nlibrary(dplyr)\n\nNow, that we know what is a package, we might wonder where exactly these packages coming from.\nThe official packages in R are located in the Comprehensive R Archive Network (CRAN). This link contains all available packages in CRAN. Additionally, there is CRAN Task Views. At the time of this writing, CRAN contains 21,606 R packages intended for various tasks.\nFurthermore, for those interested in bioinformatics, the related packages are located in the Bioconductor. At the time of this writing, Bioconductor contains 2,289 R packages related to bioinformatics.\nIn addition to CRAN and Bioconductor, there are unofficial R packages, which are usually located in GitHub and GitLab. There are probably thousands of these unofficial packages. For example, dmetar which is located in GitHub, contains R functions and codes to facilitate the conduction of meta-analyses.\nMore often than not, the official packages in CRAN also have their GitHub repositories in which they contain the latest development of R functions and codes before they going to be released in CRAN. So, current bugs and errors in the package are corrected first using the unofficial packages from GitHub or GitLab before they are eventually released to the CRAN repositories.\nSeveral packages can help in installing the unofficial packages. The two most commonly used packages are devtools and remote (or at least I commonly used them).\nFirst, we need to install the packages.\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remote\")\n\nNext, we can install the unofficial packages.\n\ndevtools::install_github(\"MathiasHarrer/dmetar\")\nremotes::install_github(\"MathiasHarrer/dmetar\")\n\nMathiasHarrer refers to the GitHub account or usually the author’s account on GitHub, followed by the name of the package. Depending on one’s preference, we can choose to use either devtools or remote.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#functions",
    "href": "basic-r.html#functions",
    "title": "4  Basics of R",
    "section": "4.7 Functions",
    "text": "4.7 Functions\nA function in R is a block of code designed to perform a specific task. It consists of an argument, in which we need to supply it. For example, the mean() function is designed to find the mean or average across the numeric values. The numeric values are the argument that we need to supply to the function.\n\nnum_values &lt;- c(5, 6, 8, 10)\nmean(num_values)\n\n[1] 7.25\n\n\nThe base R itself has numerous functions that are accessible to us. These base R functions can be used immediately once we open R (or RStudio or any other IDEs). In contrast, we also have R functions from the packages that we installed. First, we need to install the packages.\n\ninstall.packages(\"tidyverse\")\n\nWe will learn about tidyverse in a little bit. Coming on to our current installation, once you install the package there are two ways to use the function inside the package. First, by loading the package, subsequently, all the functions in the package are ready to be used by us. For example below, bind_rows() is one of the functions from the dplyr package.\n\n# Load the package\nlibrary(dplyr) \n\n# Example of functions from dplyr package\nbind_rows()\nbind_cols()\n\nSecondly, we can use :: to access a single function that we are interested in. However, in this approach, only a single function is loaded.\n\n# Example of dplyr functions\ndplyr::bind_rows()\ndplyr::bind_cols()\n\nAs we can see, every time we want to call a function from the dplyr package we type dplyr:: as we do not load the package first.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#tidyverse-package",
    "href": "basic-r.html#tidyverse-package",
    "title": "4  Basics of R",
    "section": "4.8 Tidyverse package",
    "text": "4.8 Tidyverse package\nWe have learned that R packages contain a collection of functions and R codes that we utilise once we load the packages. So, tidyverse is an opinionated collection of R packages designed for data science (Wickham et al. 2019).\n\n# List of all packages in tidyverse\ntidyverse::tidyverse_packages(include_self = FALSE) |&gt;\n  data.frame(Packages = _)\n\n        Packages\n1          broom\n2     conflicted\n3            cli\n4         dbplyr\n5          dplyr\n6         dtplyr\n7        forcats\n8        ggplot2\n9    googledrive\n10 googlesheets4\n11         haven\n12           hms\n13          httr\n14      jsonlite\n15     lubridate\n16      magrittr\n17        modelr\n18        pillar\n19         purrr\n20          ragg\n21         readr\n22        readxl\n23        reprex\n24         rlang\n25    rstudioapi\n26         rvest\n27       stringr\n28        tibble\n29         tidyr\n30          xml2\n\n\nTable 4.2 below summarises common packages in tidyverse and its uses.\n\n\n\nTable 4.2: Common packages in tidyverse.\n\n\n\n\n\n\n\n\n\nPackages\nSummary\n\n\n\n\nggplot2\nFor data visualisation.\n\n\ndplyr\nProvides tools for data manipulation, including functions for filtering, selecting, grouping, and summarizing data.\n\n\ntidyr\nSpecializes in data tidying, allowing users to transform datasets into a tidy format ready for analysis or visualisation.\n\n\nreadr\nUsed for reading rectangular data (e.g., CSV, TSV files) into R quickly and efficiently.\n\n\npurr\nIntroduces a functional programming paradigm in R with tools for applying functions to data structures like lists and vectors.\n\n\ntibble\nEnhances data frames in R by making them more user-friendly with better printing options and stricter type checking\n\n\nstringr\nFacilitates consistent handling of strings, offering functions for string manipulation, pattern matching, and transformations.\n\n\nforcats\nDesigned to work with categorical data (factors).\n\n\n\n\n\n\nTidyverse is commonly utilised for data analysis and throughout this book, we going to use tidyverse functions numerous times.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#pipe-operators",
    "href": "basic-r.html#pipe-operators",
    "title": "4  Basics of R",
    "section": "4.9 Pipe operators",
    "text": "4.9 Pipe operators\nThere are two types of pipe in R:\n\n|&gt;: this pipe is from base R, first introduced in R version 4.1.0.\n%&gt;%: this pipe is from the magrittr package, which presented tidyverse.\n\nTo use |&gt;, we do not actually need to load anything as it is already available in base R. However, to use %&gt;%, you need to load tidyverse. Certain tidyverse associated packages such as dplyr, forcats, and magrittr also load the %&gt;%.\nThe main function of these pipe operators (regardless of which one we use) is to make our R codes more readable and intuitive. So, let us compare the codes without and with the pipe.\n\nmean_value &lt;- mean(\n  subset(\n    data.frame(value = 1:10, group = rep(c(\"A\", \"B\"), each = 5)),\n    group == \"A\"\n  )$value\n)\nmean_value\n\n[1] 3\n\n\nNow, compare the codes with the pipe.\n\nmean_value &lt;- data.frame(value = 1:10, group = rep(c(\"A\", \"B\"), each = 5)) |&gt;\n  subset(group == \"A\") |&gt;\n  with(mean(value))\nmean_value\n\n[1] 3\n\n\nBasically, what we do in both R codes are:\n\nCreate a data frame with ten rows and two columns.\n\ndata.frame(value = 1:10, group = rep(c(\"A\", \"B\"), each = 5))\n\n   value group\n1      1     A\n2      2     A\n3      3     A\n4      4     A\n5      5     A\n6      6     B\n7      7     B\n8      8     B\n9      9     B\n10    10     B\n\n\n\n\n\nFilter out the group column to value A.\n\ndata.frame(value = 1:10, group = rep(c(\"A\", \"B\"), each = 5)) |&gt;\n  subset(group == \"A\")\n\n  value group\n1     1     A\n2     2     A\n3     3     A\n4     4     A\n5     5     A\n\n\nCalculate the mean.\n\nmean(c(1, 2, 3, 4, 5))\n\n[1] 3\n\n\n\nComing back to both R codes, we can intuitively see that the codes with the pipe are more readable and clear compared to the other one. The codes with pipe can be read line by line, while the codes without the pipe need to be read inside out. As you can imagine, once our codes are more complex, the less readable the codes will become.\nSo, which pipe operators to choose?\nIn most cases, the differences are not significant enough to impact your code. Therefore, you can choose the pipe operator that best suits your preference or coding style. Most users will find that both options perform similarly for general tasks, so selecting one often comes down to familiarity or ease of use.\n|&gt; is a built-in pipe operator which can be used immediately without the need to load any package. %&gt;% can be used once the tidyverse package is loaded. The latter pipe has short cut built-in in RStudio. For Windows user, Ctrl + Shift + M and for Mac user, Cmd + Shift + M. Additionally, we can set up the shortcut for |&gt; or further change the shortcut for %&gt;%, though, this will not be covered in this book.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#chapter-summary",
    "href": "basic-r.html#chapter-summary",
    "title": "4  Basics of R",
    "section": "4.10 Chapter summary",
    "text": "4.10 Chapter summary\nIn this chapter, we learn about:\n\nData, its types and structures in R.\nR packages and functions.\nPipe operators.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basic-r.html#revision",
    "href": "basic-r.html#revision",
    "title": "4  Basics of R",
    "section": "4.11 Revision",
    "text": "4.11 Revision\n\nWhat this ? actually do in R?\nList all the data types that we learn in this chapter.\nList all the data structures that we learn in this chapter.\nWhat is the difference between installing the package and loading the package?\nWhich one to choose between |&gt; and %&gt;%?\nList three packages in tidyverse and summarise their functionalities.\n\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html",
    "href": "data-wrangling.html",
    "title": "5  Data wrangling",
    "section": "",
    "text": "5.1 Load packages\nThe following packages will be used in this chapter. Please run these lines of code before proceeding to other sections.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#sec-data-wrangle-index",
    "href": "data-wrangling.html#sec-data-wrangle-index",
    "title": "5  Data wrangling",
    "section": "5.2 Indexing",
    "text": "5.2 Indexing\nIndexing involves selecting specific elements within data structures, which can be done using [].\n\n5.2.1 Vector\nFirst, let’s create a vector.\n\nvec_data &lt;- 1:10\nvec_data\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nTo select specific elements:\n\n# Select a single element\nvec_data[2]\n\n[1] 2\n\n# Select 3rd and 6th element\nvec_data[c(3,6)]\n\n[1] 3 6\n\n\n\n\n5.2.2 Data frame\nWe will use iris, a built-in dataset in R, to demonstrate indexing within a data frame. Detailed information about this dataset can be accessed by typing ?iris in the Console.\n\n?iris\n\nBelow is a summary of this dataset.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nWe will use [], where the general syntax is [row, column].\n\n# Selecting the 1st row\niris[1, ]\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n\n# Selecting the 1st and 2nd row\niris[c(1, 2), ]\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n\n\nNow let’s see how to index columns.\n\n# Selecting the 1st row\niris[, 1]\n\n# Selecting the 1st and 2nd row\niris[, c(1, 2)]\n\n\n\n[1] 5.1 4.9 4.7 4.6 5.0 5.4\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n\nBy default, this shows the entire column, but here we’ll limit the output to the first six items for clarity.\nInstead of numbers, we can also use column names:\n\n# Selecting the 1st row\niris[, \"Sepal.Length\"]\n\n# Selecting the 1st and 2nd row\niris[, c(\"Sepal.Length\", \"Sepal.Width\")]\n\n\n\n[1] 5.1 4.9 4.7 4.6 5.0 5.4\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n\nTo select both a specific row and column, we can combine what we’ve learned. For example, to select the first row and the first column:\n\n# Approach 1\niris[1, 1]\n\n[1] 5.1\n\n# Approach 2\niris[1, \"Sepal.Length\"]\n\n[1] 5.1\n\n\nTo select the first five rows and the first two columns:\n\n# Approach 1\niris[1:5, 1:2]\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n\n# Approach 2\niris[1:5, c(\"Sepal.Length\", \"Sepal.Width\")]\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n\n\nFor selecting a single column, an easier approach is to use $:\n\niris$Petal.Length\n\n\n\n[1] 1.4 1.4 1.3 1.5 1.4 1.7\n\n\n\n\n5.2.3 Selecting and slicing\nIn R, there are many ways to perform tasks. Rather than using [], dplyr provides select() and slice(), which are often preferred for their readability. The dplyr package is part of the tidyverse.\nThe select() function is used to choose specific columns.\n\n# Select a single column\niris %&gt;% \n  select(Sepal.Length)\n\n\n\n  Sepal.Length\n1          5.1\n2          4.9\n3          4.7\n4          4.6\n5          5.0\n6          5.4\n\n\n\n# Select several columns\niris %&gt;% \n  select(Sepal.Length, Sepal.Width)\n\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n\nSimilarly, slice() is used to extract specific rows.\n\n# Select a single row\niris %&gt;% \n  slice(100)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          5.7         2.8          4.1         1.3 versicolor\n\n\n\n# Select several rows\niris %&gt;% \n  slice(2, 5, 100)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          4.9         3.0          1.4         0.2     setosa\n2          5.0         3.6          1.4         0.2     setosa\n3          5.7         2.8          4.1         1.3 versicolor\n\n\nBy combining both select() and slice(), we can access specific rows and columns.\n\niris %&gt;% \n  select(Sepal.Length) %&gt;% \n  slice(1:5)\n\n  Sepal.Length\n1          5.1\n2          4.9\n3          4.7\n4          4.6\n5          5.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#filtering",
    "href": "data-wrangling.html#filtering",
    "title": "5  Data wrangling",
    "section": "5.3 Filtering",
    "text": "5.3 Filtering\nFiltering allows us to select rows based on a condition. For example, if we want to filter the Species column for the value \"setosa\" in the iris dataset, we start by creating an index:\n\nind &lt;- iris$Species == \"setosa\"\n\nNext, we apply the index to the dataset.\n\niris[ind, ]\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe == symbol is a logical operator. Table 5.1 presents the most common logical operators in R.\n\n\n\nTable 5.1: Common logical operators in R.\n\n\n\n\n\nOperators\nDescription\n\n\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n\n\n\n\nAlternatively, we can use filter() from dplyr for the same result.\n\niris %&gt;% \n  filter(Species == \"setosa\")\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nfilter() is often more readable, especially for beginners, though both methods yield the same output.\nAdditionally, we can combine multiple conditions using | (or) and & (and). For instance, to filter the iris dataset for rows where:\n\nSpecies is \"setosa\", and\nSepal.Length is greater than 5.6 cm:\n\n\n# Make an index\nind2 &lt;- iris$Species == \"setosa\" & iris$Sepal.Length &gt; 5.6\n\n# Apply the index to the dataset\niris[ind2, ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n19          5.7         3.8          1.7         0.3  setosa\n\n\nOr, with filter():\n\niris %&gt;% \n  filter(Species == \"setosa\" & Sepal.Length &gt; 5.6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.8         4.0          1.2         0.2  setosa\n2          5.7         4.4          1.5         0.4  setosa\n3          5.7         3.8          1.7         0.3  setosa",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#sorting",
    "href": "data-wrangling.html#sorting",
    "title": "5  Data wrangling",
    "section": "5.4 Sorting",
    "text": "5.4 Sorting\nSorting arranges the rows of a data frame by specific column values.\nTo demonstrate, let’s limit iris to its first ten rows for easier display. Using head() will give the top ten rows, while tail() provides the last rows.\n\niris_top10 &lt;- iris %&gt;% head(10)\niris_top10\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\nTo sort by a single column, use sort(). Setting decreasing = FALSE sorts in ascending order.\n\nsort(iris_top10$Sepal.Length, decreasing = FALSE)\n\n [1] 4.4 4.6 4.6 4.7 4.9 4.9 5.0 5.0 5.1 5.4\n\n\nAnother useful function is order(), which returns the indices and can arrange the entire dataset.\n\n# Make an index\nind3 &lt;- order(iris_top10$Sepal.Length, decreasing = FALSE)\n\n# Apply the index to the dataset\niris_top10[ind3, ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n9           4.4         2.9          1.4         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n7           4.6         3.4          1.4         0.3  setosa\n3           4.7         3.2          1.3         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n5           5.0         3.6          1.4         0.2  setosa\n8           5.0         3.4          1.5         0.2  setosa\n1           5.1         3.5          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n\n\nWith tidyverse, we have arrange(), equivalent to order() in base R:\n\niris_top10 %&gt;% \n  arrange(Sepal.Length)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           4.4         2.9          1.4         0.2  setosa\n2           4.6         3.1          1.5         0.2  setosa\n3           4.6         3.4          1.4         0.3  setosa\n4           4.7         3.2          1.3         0.2  setosa\n5           4.9         3.0          1.4         0.2  setosa\n6           4.9         3.1          1.5         0.1  setosa\n7           5.0         3.6          1.4         0.2  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           5.1         3.5          1.4         0.2  setosa\n10          5.4         3.9          1.7         0.4  setosa\n\n\nTo sort in descending order, use desc():\n\niris_top10 %&gt;% \n  arrange(desc(Sepal.Length))\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.4         3.9          1.7         0.4  setosa\n2           5.1         3.5          1.4         0.2  setosa\n3           5.0         3.6          1.4         0.2  setosa\n4           5.0         3.4          1.5         0.2  setosa\n5           4.9         3.0          1.4         0.2  setosa\n6           4.9         3.1          1.5         0.1  setosa\n7           4.7         3.2          1.3         0.2  setosa\n8           4.6         3.1          1.5         0.2  setosa\n9           4.6         3.4          1.4         0.3  setosa\n10          4.4         2.9          1.4         0.2  setosa",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#rename",
    "href": "data-wrangling.html#rename",
    "title": "5  Data wrangling",
    "section": "5.5 Rename",
    "text": "5.5 Rename\nFirst, let’s check the column names.\n\ncolnames(iris_top10)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nSuppose we want to rename the species column to type.\n\ncolnames(iris_top10)[5] &lt;- \"type\"\n\nNow, if we check the column names again:\n\ncolnames(iris_top10)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"type\"        \n\n\nAlternatively, we can use the rename() function from dplyr. Here, we will rename Sepal.Length to LengthofSepal.\n\n# Change the column name\niris_top10 &lt;- \n  iris_top10 %&gt;% \n  rename(LengthofSepal = \"Sepal.Length\")\n\n# Check the column names\ncolnames(iris_top10)\n\n[1] \"LengthofSepal\" \"Sepal.Width\"   \"Petal.Length\"  \"Petal.Width\"  \n[5] \"type\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#create-new-column",
    "href": "data-wrangling.html#create-new-column",
    "title": "5  Data wrangling",
    "section": "5.6 Create new column",
    "text": "5.6 Create new column\nLet’s create a smaller iris dataset first.\n\niris_bottom10 &lt;- tail(iris, 10)\n\nUsing base R functions, we can create a new column as follows:\n\niris_bottom10$Sepal.Lengthx2 &lt;- iris_bottom10$Sepal.Length * 2\nhead(iris_bottom10)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species Sepal.Lengthx2\n141          6.7         3.1          5.6         2.4 virginica           13.4\n142          6.9         3.1          5.1         2.3 virginica           13.8\n143          5.8         2.7          5.1         1.9 virginica           11.6\n144          6.8         3.2          5.9         2.3 virginica           13.6\n145          6.7         3.3          5.7         2.5 virginica           13.4\n146          6.7         3.0          5.2         2.3 virginica           13.4\n\n\nHere, we create a new variable Sepal.Lengthx2 by multiplying Sepal.Length by 2. Using tidyverse, we can achieve this with mutate().\n\niris_bottom10 &lt;- \n  iris_bottom10 %&gt;% \n  mutate(Sepal.Widthx2 = Sepal.Width * 2)\nhead(iris_bottom10)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species Sepal.Lengthx2\n141          6.7         3.1          5.6         2.4 virginica           13.4\n142          6.9         3.1          5.1         2.3 virginica           13.8\n143          5.8         2.7          5.1         1.9 virginica           11.6\n144          6.8         3.2          5.9         2.3 virginica           13.6\n145          6.7         3.3          5.7         2.5 virginica           13.4\n146          6.7         3.0          5.2         2.3 virginica           13.4\n    Sepal.Widthx2\n141           6.2\n142           6.2\n143           5.4\n144           6.4\n145           6.6\n146           6.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#change-data-format",
    "href": "data-wrangling.html#change-data-format",
    "title": "5  Data wrangling",
    "section": "5.7 Change data format",
    "text": "5.7 Change data format\nData can be structured in two primary formats:\n\nLong format: Each row represents a single observation.\nWide format: Each subject has one row, with variables across columns.\n\nIn long format, each row represents a single observation. This format is commonly used for data manipulation and analysis. Let’s create an example of a long-format dataset. Here, five participants were given a dietary supplement to reduce weight, and the data contains:\n\nid: participant ID\ntime: either pre- or post-supplement\nweight: participant’s weight\n\n\n# Set seed for reproducibility\nset.seed(123) \n\n# Create a long format data\ndata_long &lt;- data.frame(\n  id = rep(1:5, each = 2),        \n  time = rep(c(\"Pre\", \"Post\"), 5),   \n  weight =  sample(x = 60:80, size = 10, replace = TRUE)              \n  )\n\n# View the data\ndata_long\n\n   id time weight\n1   1  Pre     74\n2   1 Post     78\n3   2  Pre     73\n4   2 Post     62\n5   3  Pre     69\n6   3 Post     77\n7   4  Pre     70\n8   4 Post     64\n9   5  Pre     79\n10  5 Post     73\n\n\nIn wide format, each subject has a single row, with each measurement in a separate column. Here is data_long converted to wide format.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a wide format data\ndata_wide &lt;- data.frame(\n  id = 1:5,\n  Pre = sample(60:80, 5, replace = TRUE),\n  Post = sample(60:80, 5, replace = TRUE)\n  )\n\n# View the data\ndata_wide\n\n  id Pre Post\n1  1  74   77\n2  2  78   70\n3  3  73   64\n4  4  62   79\n5  5  69   73\n\n\nIn the wide format, data is often easier to interpret. Converting between long and wide formats is simple in R. To transform data_wide into long format, use pivot_longer():\n\ndata_long2 &lt;- \n  data_wide %&gt;% \n  pivot_longer(cols = 2:3, names_to = \"time\", values_to = \"weight\")\ndata_long2\n\n# A tibble: 10 × 3\n      id time  weight\n   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;\n 1     1 Pre       74\n 2     1 Post      77\n 3     2 Pre       78\n 4     2 Post      70\n 5     3 Pre       73\n 6     3 Post      64\n 7     4 Pre       62\n 8     4 Post      79\n 9     5 Pre       69\n10     5 Post      73\n\n\nFor pivot_longer(), we need to supply three arguments:\n\ncols: columns to be changed into a long format excluding the id column\nnames_to: name of a new column which consist of column names from a wide format data\nvalues_to: name of a new column which consist of values from cols\n\nTo convert data_long2 back to wide format, use pivot_wider():\n\ndata_wide2 &lt;- \n  data_long2 %&gt;% \n  pivot_wider(id_cols = \"id\", names_from = \"time\", values_from = \"weight\")\ndata_wide2\n\n# A tibble: 5 × 3\n     id   Pre  Post\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1    74    77\n2     2    78    70\n3     3    73    64\n4     4    62    79\n5     5    69    73\n\n\nTo use pivot_wider(), the three basic arguments needed are:\n\nid_cols: ID column\nnames_from: name of a column to get the column names for the wide data\nvalues_from: name of a column to get the values from\n\nBoth functions have additional arguments detailed in the Help pane (use ? in front of the function name).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#change-variable-type",
    "href": "data-wrangling.html#change-variable-type",
    "title": "5  Data wrangling",
    "section": "5.8 Change variable type",
    "text": "5.8 Change variable type\nHere are the main variable types in R:\n\n\n\nTable 5.2: Example of each of variable type in R.\n\n\n\n\n\nVariables\nExamples\n\n\n\n\nInteger\n100, 77\n\n\nNumeric\n100.2, 77.8\n\n\nCharacter\n“hello”, “ahmad”\n\n\nLogical\nTRUE, FALSE\n\n\nFactor\n“male”, “female”\n\n\nDate\n9/7/2024, 9 July 2024\n\n\n\n\n\n\nThe most common types in data are numeric, factor, and date. When importing data from software such as SPSS, STATA, or Excel, R may not always recognise the correct types. Let’s create a sample dataset and explore handling these issues.\n\n# Create a data frame with mixed-up variable types\ndata_messed_up &lt;- data.frame(\n  id = as.character(1:6),                \n  score = as.character(sample(1:100, 6)),    \n  gender = rep(c(\"Male\", \"Female\"), length.out = 6)\n  )\n\n# View the variable types\nstr(data_messed_up)\n\n'data.frame':   6 obs. of  3 variables:\n $ id    : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ score : chr  \"25\" \"90\" \"91\" \"69\" ...\n $ gender: chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n\n# View the data\ndata_messed_up\n\n  id score gender\n1  1    25   Male\n2  2    90 Female\n3  3    91   Male\n4  4    69 Female\n5  5    98   Male\n6  6    57 Female\n\n\nWe can see that score should be numeric and gender a factor. To convert these types, use as.numeric() and as.factor(). Let’s create a copy of data_messed_up for demonstration purposes.\n\ndata_messed_up2 &lt;- data_messed_up\n\nUsing base R functions, we can adjust the types:\n\n# Change score column to numeric\ndata_messed_up$score &lt;- as.numeric(data_messed_up$score)\n\n# Change gender column to factor\ndata_messed_up$gender &lt;- as.factor(data_messed_up$gender)\n\n# View variable type\nstr(data_messed_up)\n\n'data.frame':   6 obs. of  3 variables:\n $ id    : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ score : num  25 90 91 69 98 57\n $ gender: Factor w/ 2 levels \"Female\",\"Male\": 2 1 2 1 2 1\n\n\nUsing tidyverse, we can achieve the same result with mutate():\n\n# Change variable types for score and gender\ndata_messed_up2 &lt;- \n  data_messed_up2 %&gt;% \n  mutate(score = as.numeric(score),\n         gender = as.factor(gender))\n\n# View variable type\nstr(data_messed_up2)\n\n'data.frame':   6 obs. of  3 variables:\n $ id    : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ score : num  25 90 91 69 98 57\n $ gender: Factor w/ 2 levels \"Female\",\"Male\": 2 1 2 1 2 1\n\n\n\n5.8.1 Handling date\nDates can be tricky. For date variables, the standard format is YYYY-MM-DD. Using lubridate, we can work with various date formats easily. Let’s look at a few examples:\n\n# Using base R \ndate_data &lt;- as.Date(\"2024-11-30\")\nstr(date_data)\n\n Date[1:1], format: \"2024-11-30\"\n\n# Using lubridate\ndate_data2 &lt;- as_date(\"2024-11-30\")\nstr(date_data2)\n\n Date[1:1], format: \"2024-11-30\"\n\n\nFor non-standard formats, use lubridate functions ymd(), dmy(), and mdy().\n\n# DD-MM-YYYY\ndate_data3 &lt;- dmy(\"30-11-2024\")\nstr(date_data3)\n\n Date[1:1], format: \"2024-11-30\"\n\n# MM-DD-YY\ndate_data4 &lt;- mdy(\"11-30-2024\")\nstr(date_data4)\n\n Date[1:1], format: \"2024-11-30\"\n\n\nCorrectly formatted date variables allow for operations such as:\n\nDate calculation: adding days to a date.\n\ndate_data4 + 7 \n\n[1] \"2024-12-07\"\n\n\nExtracting date components usch as month and year.\n\n# Extract month\nmonth(date_data4)\n\n[1] 11\n\n# Extract year\nyear(date_data4)\n\n[1] 2024\n\n\n\nTo demonstrate dates in a dataset, we’ll recreate data_messed_up with a date column.\n\n# Create a data frame with mixed-up variable types\ndata_messed_up &lt;- data.frame(\n  id = as.character(1:6),                \n  score = as.character(sample(1:100, 6)),    \n  gender = rep(c(\"Male\", \"Female\"), length.out = 6),\n  date = as.character(Sys.Date() - 1:6)\n  )\n\n# View variable type\nstr(data_messed_up)\n\n'data.frame':   6 obs. of  4 variables:\n $ id    : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ score : chr  \"92\" \"9\" \"93\" \"72\" ...\n $ gender: chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n $ date  : chr  \"2024-12-07\" \"2024-12-06\" \"2024-12-05\" \"2024-12-04\" ...\n\n\nThe date column is recognised as a character. To convert date to a date format, use as_date().\n\n# Change variable types for score, gender, and date\ndata_messed_up &lt;- \n  data_messed_up %&gt;% \n  mutate(score = as.numeric(score),\n         gender = as.factor(gender),\n         date = as_date(date))\n\n# View variable type\nstr(data_messed_up)\n\n'data.frame':   6 obs. of  4 variables:\n $ id    : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ score : num  92 9 93 72 26 7\n $ gender: Factor w/ 2 levels \"Female\",\"Male\": 2 1 2 1 2 1\n $ date  : Date, format: \"2024-12-07\" \"2024-12-06\" ...\n\n\nThe simplest solution for the date issue is to make sure we properly input the date according to the right format (YYYY-MM-DD) during the data collection process properly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#merge-datasets",
    "href": "data-wrangling.html#merge-datasets",
    "title": "5  Data wrangling",
    "section": "5.9 Merge datasets",
    "text": "5.9 Merge datasets\nIf you collect data from different sources, you may need to combine datasets by row or column. rbind() combines datasets by row, while cbind() combines them by column.\nLet us see how to combine two iris datasets.\n\n# Data collected from area A\ndata1 &lt;- iris\n\n# Data collected from area B\ndata2 &lt;- iris\n\n# Combine both datasets by a row\ndata_combined_row &lt;- rbind(data1, data2)\n\nNow, we can check the dimensions of the data. The first element represents a row and the second element represents the column.\n\n# Dimension of data1\ndim(data1)\n\n[1] 150   5\n\n# Dimension of data2\ndim(data2)\n\n[1] 150   5\n\n# Dimension of the combined data\ndim(data_combined_row)\n\n[1] 300   5\n\n\nWe can see that the rows of data1 and data2 each are 150. Combining both data by a row gives us 300 rows. It is to be noted that to use rbind(), both data should have the same column numbers and names. Additionally, rbind() is not limited to two data only.\nNext, let us see the cbind(). Using the same data, we can combine both data by a column.\n\n# Combine both datasets by a column\ndata_combined_col &lt;- cbind(data1, data2)\n\nThus, by further checking the dimension, we can see that the total column of data_combined_col is 10, which is the sum of 5 columns in each of the data1 and data2, while the row remained the same.\n\n# Dimension of data1\ndim(data1)\n\n[1] 150   5\n\n# Dimension of data2\ndim(data2)\n\n[1] 150   5\n\n# Dimension of the combined data\ndim(data_combined_col)\n\n[1] 150  10\n\n\nHowever, similar to rbind(), to use the cbind(), the rows of both data should be identical. If participant A is in the first row of data1, the, in data2, participant A should also be in the first row.\nThere is another set of functions that is more efficient than cbind(), in which we can combine two or more datasets according to the id. Let us create two datasets that are related and have the same ID.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create the first half of the data\ndata_half1 &lt;- data.frame(\n  name = c(\"Ahmad\", \"Ali\", \"Cheng\", \"Rama\", \"Wei\"),\n  height_cm = sample(160:180, 5, replace = FALSE)\n  )\n\n# Create the second half of the data\ndata_half2 &lt;- data.frame(\n  name = c(\"Ahmad\", \"Ali\", \"Cheng\", \"Rama\", \"Karim\"),\n  weight_kg = sample(70:90, 5, replace = FALSE)\n  )\n\n# The first dataset\ndata_half1\n\n   name height_cm\n1 Ahmad       174\n2   Ali       178\n3 Cheng       173\n4  Rama       162\n5   Wei       169\n\n# The second dataset\ndata_half2\n\n   name weight_kg\n1 Ahmad        87\n2   Ali        80\n3 Cheng        74\n4  Rama        83\n5 Karim        88\n\n\nNotice that the last row of both datasets is not similar. To combine both datasets, we can use either left_join() or right_join(). Both produce the same result. If there is a mismatch between the two datasets, left_join() will keep the first dataset as a reference, and any of the id of the second dataset that does not match the first one will be removed. right_join() works similar to the left_join() but in the opposite.\n\n# Combine the data\ndata_full_left &lt;- \n  data_half1 %&gt;% #first datasets\n  left_join(data_half2, by = \"name\") #second datasets\n\n# View the combined data\ndata_full_left\n\n   name height_cm weight_kg\n1 Ahmad       174        87\n2   Ali       178        80\n3 Cheng       173        74\n4  Rama       162        83\n5   Wei       169        NA\n\n\nWe can see that karim in the second dataset is excluded. Let’s see what will happen if we use right_join().\n\n# Combine the data\ndata_full_right &lt;- \n  data_half1 %&gt;% #first datasets\n  right_join(data_half2, by = \"name\") #second datasets\n\n# View the combined data\ndata_full_right\n\n   name height_cm weight_kg\n1 Ahmad       174        87\n2   Ali       178        80\n3 Cheng       173        74\n4  Rama       162        83\n5 Karim        NA        88\n\n\nWe can see that Wei in the first dataset is excluded. To include all the participants, despite the mismatch of the id is by using full_join().\n\n# Combine the data\ndata_full &lt;- \n  data_half1 %&gt;% \n  full_join(data_half2, by = \"name\") \n\n# View the combined data\ndata_full\n\n   name height_cm weight_kg\n1 Ahmad       174        87\n2   Ali       178        80\n3 Cheng       173        74\n4  Rama       162        83\n5   Wei       169        NA\n6 Karim        NA        88\n\n\nBoth Wei and Karim are kept in the combined dataset. Additionally, there is inner_join(), which will exclude both Wei and Karim. This function keeps the data that the name is present in both datasets only.\n\n# Combine the data\ndata_full_inner &lt;- \n  data_half1 %&gt;% \n  inner_join(data_half2, by = \"name\") \n\n# View the combined data\ndata_full_inner\n\n   name height_cm weight_kg\n1 Ahmad       174        87\n2   Ali       178        80\n3 Cheng       173        74\n4  Rama       162        83",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#chapter-summary",
    "href": "data-wrangling.html#chapter-summary",
    "title": "5  Data wrangling",
    "section": "5.10 Chapter summary",
    "text": "5.10 Chapter summary\nIn this chapter, we covered the most common and basic operations in data wrangling. More operations were not covered in this chapter as this book is intended for beginners.\nTo summarise, we have covered:\n\nHow to select a column and a row\nHow to filter a dataset based on a condition applied to columns\nHow to rename and create a column\nHow to manage variable types\nHow to combine several datasets into one",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#revision",
    "href": "data-wrangling.html#revision",
    "title": "5  Data wrangling",
    "section": "5.11 Revision",
    "text": "5.11 Revision\n\nLoad mtcars dataset in R.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nRead aboutmtcars.\n\n?mtcars\n\nHow many cars have mpg &gt; 25?\nHow many cars have mpg &gt; 25 and a number of carburetors of 2?\nHow many cars have a V-shaped engine?\nChange vs and am into a factor.\nChange the wide format data below into a long or tidy format.\n\n# Set seed for reproducibility\nset.seed(123)  \n\n# Create a wide format data\ndata_wide &lt;- data.frame(\n  id = 1:10,\n  time1 = sample(1:100, 10, replace = TRUE),\n  time2 = sample(1:100, 10, replace = TRUE),\n  time3 = sample(1:100, 10, replace = TRUE), \n  age = sample(18:80, 10, replace = TRUE)\n  )\n\n# View the data frame\nhead(data_wide)\n\n  id time1 time2 time3 age\n1  1    31    90    26  32\n2  2    79    91     7  49\n3  3    51    69    42  59\n4  4    14    91     9  62\n5  5    67    57    83  24\n6  6    42    92    36  26\n\n\nThe result of the long format data should appear like this.\n\n\n\n\n\nid\nage\ntime_points\ntime_minute\n\n\n\n\n1\n32\ntime1\n31\n\n\n1\n32\ntime2\n90\n\n\n1\n32\ntime3\n26\n\n\n2\n49\ntime1\n79\n\n\n2\n49\ntime2\n91\n\n\n2\n49\ntime3\n7\n\n\n3\n59\ntime1\n51\n\n\n3\n59\ntime2\n69\n\n\n3\n59\ntime3\n42\n\n\n4\n62\ntime1\n14\n\n\n4\n62\ntime2\n91\n\n\n4\n62\ntime3\n9\n\n\n5\n24\ntime1\n67\n\n\n5\n24\ntime2\n57\n\n\n5\n24\ntime3\n83\n\n\n6\n26\ntime1\n42\n\n\n6\n26\ntime2\n92\n\n\n6\n26\ntime3\n36\n\n\n7\n58\ntime1\n50\n\n\n7\n58\ntime2\n9\n\n\n7\n58\ntime3\n78\n\n\n8\n27\ntime1\n43\n\n\n8\n27\ntime2\n93\n\n\n8\n27\ntime3\n81\n\n\n9\n40\ntime1\n14\n\n\n9\n40\ntime2\n99\n\n\n9\n40\ntime3\n43\n\n\n10\n44\ntime1\n25\n\n\n10\n44\ntime2\n72\n\n\n10\n44\ntime3\n76",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "6  Data visualisation",
    "section": "",
    "text": "6.1 Load packages\nPlease load a tidyverse package before proceeding to the next section. We do not need to load the ggplot2 package as it is part of the tidyverse meta package.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#data",
    "href": "plotting.html#data",
    "title": "6  Data visualisation",
    "section": "6.2 Data",
    "text": "6.2 Data\nIn this chapter, we going to utilise ChickWeight a built-in dataset in R. This dataset is about the effect of different diet regimens on the weight of the chicks.\n\n# More info about the ChickWeight data\n?ChickWeight\ndata(\"ChickWeight\")\n\nLet’s create a new variable from the Time variable. In this dataset, the weight of the chicks was weighted each day until 21 days.\n\nchick_data &lt;- \n  ChickWeight %&gt;% \n  mutate(Time_group = cut(Time, breaks = 3, labels = c(\"Period 1\", \"Period 2\", \"Period 3\")))\n\nWe can check the variables.\n\nstr(chick_data)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  5 variables:\n $ weight    : num  42 51 59 64 76 93 106 125 149 171 ...\n $ Time      : num  0 2 4 6 8 10 12 14 16 18 ...\n $ Chick     : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 15 15 15 15 15 15 15 ...\n $ Diet      : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Time_group: Factor w/ 3 levels \"Period 1\",\"Period 2\",..: 1 1 1 1 2 2 2 2 3 3 ...\n\n\nThe second data we going to use is ToothGrowth. The data is about the effect of vitamin C on tooth growth in guinea pigs.\n\n# More info about the data\n?ToothGrowth\ndata(\"ToothGrowth\")\n\nLet’s create a new variable based variable dose.\n\ntooth_data &lt;- \n  ToothGrowth %&gt;% \n  mutate(dose_group = case_when(dose == 0.5 ~ \"low\",\n                                dose == 1 ~ \"intermediate\",\n                                .default = \"high\"),\n         dose_group = as.factor(dose_group))\n\nHere, we use case_when() to classify the dose into 3 groups:\n\n0.5 as low dose\n1 as intermediate dose\n2 as high dose\n\n\n# Dose variable\ntable(tooth_data$dose)\n\n\n0.5   1   2 \n 20  20  20 \n\n# Dose group variable\ntable(tooth_data$dose_group)\n\n\n        high intermediate          low \n          20           20           20 \n\n\nThen, we can check the variable types.\n\nstr(tooth_data)\n\n'data.frame':   60 obs. of  4 variables:\n $ len       : num  4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ...\n $ supp      : Factor w/ 2 levels \"OJ\",\"VC\": 2 2 2 2 2 2 2 2 2 2 ...\n $ dose      : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ dose_group: Factor w/ 3 levels \"high\",\"intermediate\",..: 3 3 3 3 3 3 3 3 3 3 ...\n\n\nLastly, the third data that we going to use in this chapter is the cars dataset, another built-in dataset in R. The data is about the speed of the cars and the distances taken by the cars to stop.\n\n# More info about the data\n?cars\ndata(\"cars\")\n\nNext, we going to create a new variable, a type of car.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a new variable\ncars_data &lt;- \n  cars %&gt;% \n  mutate(car = sample(c(\"A\", \"B\", \"C\"), size = 50, replace = TRUE),\n         car = as.factor(car))\n\nWe can check the new variable by using table().\n\ntable(cars_data$car)\n\n\n A  B  C \n16 15 19 \n\n\nAlso, we need to check the variable type.\n\nstr(cars_data)\n\n'data.frame':   50 obs. of  3 variables:\n $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\n $ car  : Factor w/ 3 levels \"A\",\"B\",\"C\": 3 3 3 2 3 2 2 2 3 1 ...",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#barplot",
    "href": "plotting.html#barplot",
    "title": "6  Data visualisation",
    "section": "6.3 Barplot",
    "text": "6.3 Barplot\nBarplot is utilised when a variable is categorical. Let’s explore the ChickWeight data. To use barplot(), the data should be a matrix.\n\ndiet_table &lt;- \n  chick_data %&gt;% \n  select(Diet) %&gt;% \n  table()\n\nNext, we can plot the Diet information.\n\nbarplot(diet_table)\n\n\n\n\n\n\n\n\nLet’s beautify the plot by setting up a colour, axis label and title. The arguments needed for those are:\n\ncol: specifies the colour.\nxlab: name of the x-axis.\nylab: name of the y-axis.\nmain: the title of the barplot.\n\n\nbarplot(diet_table, col = \"steelblue\", \n        xlab = \"Diet regimen\", \n        ylab = \"Number of chicks\", \n        main = \"Protein diet regimens received by the chicks\")\n\n\n\n\n\n\n\n\nNow, let’s do this in ggplot2. In ggplot2, the R codes are stacked on one over another. ggplot() initialises a ggplot object, and it sort of sets up the canvas for the plot. This part is aaplies to all the plots in ggplot2.\n\nggplot()\n\n\n\n\n\n\n\n\nNext, we specify what type of plots that we want. Here, by using geom_bar(), we can see that the x- and y-axis are labelled.\n\nggplot() +\n  geom_bar()\n\n\n\n\n\n\n\n\nLet’s supply the data for the barplot. The x argument specifies the column on the x-axis that we want to plot.\n\nchick_data %&gt;% \n  ggplot(aes(x = Diet)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe can further beautify the plot by adding colour, labelling the axis, putting a title, and changing the theme of the plot. The arguments needed are:\n\nfill: specifies the colour.\nx: name of the x-axis.\ny: name of the y-axis.\ntitle: the title of the barplot.\ntheme_bw(): specify the theme of the barplot.\n\n\nchick_data %&gt;% \n  ggplot(aes(x = Diet)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Protein diet regimens received by the chicks\", \n       x = \"Diet regimen\",\n       y = \"Number of chicks\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can further add another variable, Time_group, that we created earlier. We remove the colour argument and add these arguments:\n\nfill in ggplot(): another group variable (must be a factor).\nposition: dodge position allows us to maintain the vertical position of the plot.\nfill in labs(): the legend title.\n\n\nchick_data %&gt;% \n  ggplot(aes(x = Diet, fill = Time_group)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Protein diet regimens received by the chicks\", \n       x = \"Diet regimen\",\n       y = \"Number of chicks\",\n       fill = \"Time period:\") +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#histogram",
    "href": "plotting.html#histogram",
    "title": "6  Data visualisation",
    "section": "6.4 Histogram",
    "text": "6.4 Histogram\nA histogram is used when a variable is numerical. It reflects the frequency distribution of the data.\nLet’s do the histogram in base R.\n\nhist(chick_data$weight)\n\n\n\n\n\n\n\n\nWe can see that from the histogram, most chicks in our data weigh around 50-100 grams regardless of time and diet regimen. We can further beautify the histogram by adding a colour, and naming the x-axis and the title. The arguments are similar to the barplot previously.\n\nhist(chick_data$weight,\n     xlab = \"Weight in grams\",\n     ylab = \"Frequency of the chicks\",\n     main = \"Histogram of the weight of the chicks (grams)\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nNext, let’s do the basic histogram in ggplot2 using geom_histogram(). The bins specifies the number of intervals used to group the data. By bins = 50, we divide the data into 50 equally spaced intervals or bins.\n\nchick_data %&gt;% \n  ggplot(aes(x = weight)) +\n  geom_histogram(bins = 50)\n\n\n\n\n\n\n\n\nWe can add a colour, title, and axis labels, and change the theme.\n\nchick_data %&gt;% \n  ggplot(aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"lightgreen\") +\n  labs(title = \"Histogram of the weight of the chicks (grams)\",\n       x = \"Weight in grams\",\n       y = \"Frequency of the chicks\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also add another variable (a factor variable). Let’s say we want to see the distribution of the chick’s weight across different diet regimens. However, instead of displaying all groups of diet regimens in a single plot, we display it across different plots in rows. facet_grid() specifies which column to use to divide across the rows.\n\nchick_data %&gt;% \n  ggplot(aes(x = weight)) +\n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of the weight of the chicks (grams)\",\n       x = \"Weight in grams\",\n       y = \"Frequency of the chicks\") +\n  facet_grid(rows = vars(Diet)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, we can see more clearly that the weight of the chicks in diet regimen 1 is skewed to the left side, while the weight of the chicks in diet regimen 4 is distributed more uniformly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#boxplot",
    "href": "plotting.html#boxplot",
    "title": "6  Data visualisation",
    "section": "6.5 Boxplot",
    "text": "6.5 Boxplot\nBoxplot is utilised when a variable is numerical. The main use of the boxplot is to display the spread of the data and further identify outliers and extreme values. Outliers are observations that lie far from the majority of the data. An extreme value may not be an outlier, but an outlier is always an extreme value.\nLet’s plot the boxplot using base R.\n\nboxplot(tooth_data$len)\n\n\n\n\n\n\n\n\nWe can further beautify the plot.\n\nboxplot(tooth_data$len,\n        col = \"orange\",\n        main = \"Boxplot of the tooth length of the guinea pigs\")\n\n\n\n\n\n\n\n\nLet’s do a similar boxplot using ggplot2.\n\ntooth_data %&gt;% \n  ggplot(aes(y = len)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTo beautify the plot.\n\ntooth_data %&gt;% \n  ggplot(aes(y = len)) +\n  geom_boxplot(fill = \"orange\") +\n  labs(title = \"Boxplot the tooth length of the guinea pigs\",\n       y = \"Length of tooth\")\n\n\n\n\n\n\n\n\nWe can further divide the boxplots by dose_group.\n\ntooth_data %&gt;% \n  ggplot(aes(x = dose_group, y = len)) +\n  geom_boxplot(fill = \"orange\") +\n  labs(title = \"Boxplot the tooth length of the guinea pigs\",\n       y = \"Length of tooth\",\n       x = \"Dose groups\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see that we may have an outlier in a low-dose group.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#violin-plot",
    "href": "plotting.html#violin-plot",
    "title": "6  Data visualisation",
    "section": "6.6 Violin plot",
    "text": "6.6 Violin plot\nA violin plot allows us to visualise the distribution of the numeric variable according to a factor variable. It is a combination of a boxplot and a density plot. In contrast to the boxplot, the violin plot needs two variables.\nThere is no function in base R to plot the violin plot. However, we can use ggplot2 to plot the violin plot.\n\ntooth_data %&gt;% \n  ggplot(aes(x = len, y = dose_group)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nWe can further beautify the plot.\n\ntooth_data %&gt;% \n  ggplot(aes(x = len, y = dose_group)) +\n  geom_violin(fill = \"pink\") +\n  labs(title = \"Violin plot of the tooth length based on the dose group\",\n       y = \"Dose group\",\n       x = \"Length of the tooth\")\n\n\n\n\n\n\n\n\nWe can further add the boxplot on top of the violin plot.\n\ntooth_data %&gt;% \n  ggplot(aes(x = len, y = dose_group)) +\n  geom_violin(fill = \"pink\", alpha = 0.4) +\n  geom_boxplot(width = 0.1, color = \"red\") +\n  labs(title = \"Violin plot of the tooth length based on the dose group\",\n       y = \"Dose group\",\n       x = \"Length of the tooth\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe argument width specifies the width of the boxplot and alpha specifies the colour density.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#scatter-plot",
    "href": "plotting.html#scatter-plot",
    "title": "6  Data visualisation",
    "section": "6.7 Scatter plot",
    "text": "6.7 Scatter plot\nScatter plots allow us to plot two continuous variables.\nLet’s plot a scatter plot using base R. We going to use the last dataset, cars_data.\n\nplot(x = cars_data$speed, y = cars_data$dist)\n\n\n\n\n\n\n\n\nWe can see that as the speed increases, the distance taken to stop also increases. Next, to beautify the plot, we name the axis and title, and specify a colour.\n\nplot(x = cars_data$speed, \n     y = cars_data$dist,\n     col = \"red\",\n     main = \"Speed vs. stopping distance\",\n     ylab = \"Speed (Mile/hour)\",\n     xlab = \"Stopping distance (ft)\")\n\n\n\n\n\n\n\n\nNow, let’s do the plot using ggplot2.\n\ncars_data %&gt;% \n  ggplot(aes(x = dist, y = speed)) +\n  geom_point()\n\n\n\n\n\n\n\n\nSimilarly to the plot in base R, we can further beautify the scatter plot.\n\ncars_data %&gt;% \n  ggplot(aes(x = dist, y = speed)) +\n  geom_point(colour = \"red\") +\n  labs(title = \"Speed vs. stopping distance\",\n       y = \"Speed (Mile/hour)\",\n       x = \"Stopping distance (ft)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nLastly, we further add another variable.\n\ncars_data %&gt;% \n  ggplot(aes(x = dist, y = speed, colour = car)) +\n  geom_point() +\n  labs(title = \"Speed vs. stopping distance\",\n       y = \"Speed (Mile/hour)\",\n       x = \"Stopping distance (ft)\",\n       colour = \"Type of car:\") +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#chapter-summary",
    "href": "plotting.html#chapter-summary",
    "title": "6  Data visualisation",
    "section": "6.8 Chapter summary",
    "text": "6.8 Chapter summary\nIn this chapter, we have learned about five different plots.\n\n\n\nTable 6.1: Commonly used plots in data analysis.\n\n\n\n\n\n\n\n\n\n\nPlots\nVariables\nApplication\n\n\n\n\nBarplot\n1 factor variable\nDisplay the proportion of a factor variable\n\n\nHistogram\n1 numerical variable\n\nDisplay the frequency distribution of a numerical variable\nIdentify the pattern of the data (skewed or normally distributed)\n\n\n\nBoxplot\n1 numerical variable\n\nDisplay the spread of distribution of a numerical variable\nIdentify extreme values and outliers\n\n\n\nViolin plot\n1 numerical variable vs. 1 factor variable\nDisplay the frequency distribution of a numerical variable based on another factor variable\n\n\nScatter plot\n1 numerical variable vs. 1 numerical variable\nDisplay a relationship between two numerical variables",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "plotting.html#revision",
    "href": "plotting.html#revision",
    "title": "6  Data visualisation",
    "section": "6.9 Revision",
    "text": "6.9 Revision\n\nLoad and read about mtcars dataset from base R.\n\n# Load the data\ndata(\"mtcars\")\n\n# Read about the data\n?mtcars\n\n\nCreate a histogram using base R for the mpg variable using base R. The plot should look like the one below.\n\n\n\n\n\n\n\n\n\nCreate a boxplot for the wt variable using base R. The plot should look like the one below.\n\n\n\n\n\n\n\n\n\nCreate a scatter plot for mpg vs. wt variables using base R. The plot should look like the one below.\n\nplot(mtcars$mpg, mtcars$wt, \n     col = \"purple\", \n     main = \"Weight vs. miles per gallon\",\n     xlab = \"Mile per gallon\",\n     ylab = \"Weight (1000 lbs)\")\n\n\n\n\n\n\n\n\n\nLoad and read about diamonds data from the ggplot2 package.\n\n# Load the packages\nlibrary(tidyverse)\n\n# Load the data\ndata(\"diamonds\")\n\n# Read about the data\n?diamonds\n\n\nCreate a barplot of the color variable using ggplot2. The plot should look like the one below.\n\n\n\n\n\n\n\n\n\nCreate a scatter plot between carat and price using ggplot2. The plot should look like the one below.\n\n\n\n\n\n\n\n\n\nCreate a violin plot between price and color and further separate it by cut The plot should look like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html",
    "href": "efficient-coding.html",
    "title": "7  Efficient coding",
    "section": "",
    "text": "7.1 Load packages\nPlease load a tidyverse package before moving to the next section.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html#loop",
    "href": "efficient-coding.html#loop",
    "title": "7  Efficient coding",
    "section": "7.2 Loop",
    "text": "7.2 Loop\nA loop is utilised in the case of an iterative process. Generally, there are two types of loops:\n\nFor loop\nWhile loop\n\nA for loop iterate iterates over a sequence. The general structure of the for loop is:\n\nfor (variable in sequence) {\n    # Code to execute\n}\n\nFor example, if we want to add 1 over a sequence of numbers:\n\n# Number sequence 1 to 10\nnum_seq &lt;- 1:10\nnum_seq\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n# Make a for loop that adds 1 to each number in the sequence\nfor (i in num_seq) {\n  i = i + 1\n  print(i)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n\n\ni reflects each number in the number sequence, while print() will print each number after 1 is added.\nNow, let’s try something more related to the data analysis. We going to use the iris dataset for this example. For those who might not be familiar, the iris dataset, is a built-in dataset in R. Further details can be read on the Help pane by typing ?iris.\nWe going to calculate the mean for each numeric column in the iris dataset.\n\n# Loop through numeric columns of the iris dataset\nfor (col in names(iris)[1:4]) {\n  \n    # Calculate the mean of the current column\n    column_mean &lt;- mean(iris[[col]])\n    \n    # Round the mean to 2 decimal points\n    column_mean &lt;- round(column_mean, digits = 2)\n    \n    # Print the column name and its mean\n    print(paste(\"Mean of\", col, \"is\", column_mean))\n}\n\n[1] \"Mean of Sepal.Length is 5.84\"\n[1] \"Mean of Sepal.Width is 3.06\"\n[1] \"Mean of Petal.Length is 3.76\"\n[1] \"Mean of Petal.Width is 1.2\"\n\n\nAdditionally, we can use for loop to do several plots. For example, we can plot a boxplot for each numeric column in the iris dataset.\n\n# Select numeric columns only from iris\ndf &lt;- iris[, -5]\n\n# Loop through numeric columns of the iris dataset\nfor (col_name in names(df)) {\n  \n  # Create a boxplot\n  boxplot(df[[col_name]], \n          main = paste(\"Boxplot of\", col_name), \n          ylab = col_name)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus, integrating a for loop in the data analysis code will make it more efficient. Next, let’s see what is a while loop.\nThe while loop executes a block of code as long as a specified condition remains true. A general structure of the while loop is:\n\nwhile (condition) {\n    # Code to execute\n}\n\nAs a basic example of the while loop, we can add 1 to the sequence of numbers as long as the number is below 10.\n\n# Initialise the number\ncount &lt;- 0\n\n# A while loop that adds 1 to each number as long as the number is less than 10\nwhile (count &lt; 10) {\n  count &lt;- count + 1\n  print(count)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nSo, as long as the count is not equal to 10, the while loop will keep adding 1 to the count. Next, let’s try the while loop which is more related to the data analysis. By using the iris dataset, let’s say we want to calculate the cumulative sum of Sepal.Length until the sum exceeds 60.\n\n# Initialize variables\nindex &lt;- 1 #index for looping\ncumulative_sum &lt;- 0 #variable to hold the cumulative sum\n\n# While loop to calculate the cumulative sum\nwhile (cumulative_sum &lt;= 60 && index &lt;= nrow(iris)) {    \n  \n  # Calculate the cumulative sum starting with the 1st row\n  cumulative_sum &lt;- cumulative_sum + iris$Sepal.Length[index]\n  \n  # Move to the next row\n  index &lt;- index + 1  \n  \n  # Display the result\n  print(paste0(\"Row \", index - 1, \", Cumulative sum: \", cumulative_sum))\n}\n\n[1] \"Row 1, Cumulative sum: 5.1\"\n[1] \"Row 2, Cumulative sum: 10\"\n[1] \"Row 3, Cumulative sum: 14.7\"\n[1] \"Row 4, Cumulative sum: 19.3\"\n[1] \"Row 5, Cumulative sum: 24.3\"\n[1] \"Row 6, Cumulative sum: 29.7\"\n[1] \"Row 7, Cumulative sum: 34.3\"\n[1] \"Row 8, Cumulative sum: 39.3\"\n[1] \"Row 9, Cumulative sum: 43.7\"\n[1] \"Row 10, Cumulative sum: 48.6\"\n[1] \"Row 11, Cumulative sum: 54\"\n[1] \"Row 12, Cumulative sum: 58.8\"\n[1] \"Row 13, Cumulative sum: 63.6\"\n\n\nThe conditions in the while loop above are:\n\nThe loop will run until the cumulative sum exceeds 60\nOr the last row of the dataset is reached.\n\nThe last line of the codes is just to print the result. For now, you do not actually need to understand it.\nGenerally, the for loop is more popular and commonly used compared to the while loop. However, knowing both loops, at least at the basic level will definitely benefit you in future. Additionally, the loop functions are not only available in R but in other programming software such as Python, Julia, MATLAB and Stata.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html#apply",
    "href": "efficient-coding.html#apply",
    "title": "7  Efficient coding",
    "section": "7.3 Apply",
    "text": "7.3 Apply\nThe apply family function in R is utilised to apply a specific operation over elements of data structures such as vectors, matrices, and data frames. This function is relatively similar to the loop function. However, in R, generally, the apply family functions are faster and more efficient compared to the loop functions.\nThere are 7 types of apply family functions:\n\napply(): applies a function over rows or columns of a matrix or array.\nlapply(): applies a function to each element of a vector, data frame or list and returns a list.\nsapply(): similar to lapply() but tries to simplify the result (e.g., into a vector or matrix).\nvapply(): similar to sapply(), but requires specifying the output type.\nmapply(): multivariate version of sapply(), applying a function to multiple arguments.\ntapply(): applies a function over subsets of a vector grouped by a factor.\nrapply(): recursive version of lapply() for nested lists.\n\nWe are not going to cover all the apply family functions in this book, but we going to cover the top three apply family functions (apply(), lapply(), sapply()).\napply() function\nThe apply() is used to apply the function over rows or columns. The basic syntax is:\n\napply(X, MARGIN, FUN, ...)\n\nThe basic arguments that we need to supply:\n\nX: an array.\nMARGIN: where the function should be applied, 1 is at row, and 2 is at column.\nFUN: the function.\n\nLet’s try applying this function to our iris dataset. Let’s say we want to calculate the mean for each numeric column in the dataset.\n\napply(iris %&gt;% select(-Species), MARGIN = 2 , FUN = mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nMARGIN = 2 indicates we want the mean of columns. If we set MARGIN = 1 means that we want the mean of each row.\nlapply() function\nThe lapply() applies a function to each element of a list and returns a list. The basic syntax is:\n\nlapply(X, FUN)\n\nWhere:\n\nX: a vector, data frame or a list.\nFUN: the function.\n\nAs an example of lapply(), we can find the mean of each column and see the result is returned in a list format.\n\nlapply(iris %&gt;% select(-Species), FUN = mean)\n\n$Sepal.Length\n[1] 5.843333\n\n$Sepal.Width\n[1] 3.057333\n\n$Petal.Length\n[1] 3.758\n\n$Petal.Width\n[1] 1.199333\n\n\nsapply() function\nThe sapply() function is similar to the lapply(), but it simplifies the result. The syntax is similar to the lapply().\nLet’s use the same example as in the lapply() section and see how is the output is formatted.\n\nsapply(iris %&gt;% select(-Species), FUN = mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nThe output is formatted in a more simplified, and in this case, the output format is similar to the one in the apply() function section.\nSo, we have seen three functions from the apply family functions. As the example in the for loop, we can also use the apply family function to plot several plots.\n\n7.3.1 purrr\npurrr package is part of tidyverse, which contains many functions that are equivalent to base R apply family functions.\nThe equivalent of lapply() function is map().\n\nmap(iris %&gt;% select(-Species), .f = mean)\n\n$Sepal.Length\n[1] 5.843333\n\n$Sepal.Width\n[1] 3.057333\n\n$Petal.Length\n[1] 3.758\n\n$Petal.Width\n[1] 1.199333\n\n\nThere are many more functions in the purrr package that are highly beneficial to learn. However, most of these functions are more advanced and require a deeper understanding of R, which goes beyond what is covered in this book. As such, they may not be suitable for beginners and novices at this stage.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html#sec-loop-function",
    "href": "efficient-coding.html#sec-loop-function",
    "title": "7  Efficient coding",
    "section": "7.4 Function",
    "text": "7.4 Function\nOne of the flexibility in R is we can make our own function. Let’s make a basic function that adds two numbers.\n\nadd_num &lt;- function(number1, number2) {\n  result &lt;- number1 + number2\n  return(result)\n}\n\nNext, let’s try the function.\n\nadd_num(number1 = 100, number2 = 200)\n\n[1] 300\n\n\nLet’s upgrade our function, instead of adding the number, we going to calculate the mean of the numbers.\n\navg_num &lt;- function(x_range) {\n  \n  # Sum of all elements in the vector\n  sum_x &lt;- sum(x_range)  \n  \n  # Determine the number of elements in the vector\n  n &lt;- length(x_range)  \n  \n  # Calculate mean\n  mean_value &lt;- sum_x / n  \n  \n  # Return the mean\n  return(mean_value)\n}\n\nLet’s try our function using the iris dataset, and compare it with the mean() in the base R.\n\n# Our function\navg_num(iris$Sepal.Length)\n\n[1] 5.843333\n\n# Mean function in R\nmean(iris$Sepal.Length)\n\n[1] 5.843333\n\n\nAdditionally, we can also integrate the R functions in our own function. Let’s say we want to build a function that can return the value of mean, standard deviation (SD), median, and interquartile range (IQR). Instead of typing the function one by one every time we need it, we can create a function that gives us the four statistical measures. So, it is more efficient to create this function if we need to run it more than two times. For those who are not familiar with these statistical measures, you can just ignore them for now as we will cover it in the later chapter.\n\nsummary_func &lt;- function(x_range) {\n  \n  # Calculate mean and standard deviation, then round the decimal points to 2\n  mean_val &lt;- mean(x_range) |&gt; round(digits = 2)\n  std_val &lt;- sd(x_range) |&gt; round(digits = 2)\n  \n  # Calculate median and interquartile range, then round the decimal points to 2\n  med_val &lt;- median(x_range) |&gt; round(digits = 2)\n  iqr_val &lt;- IQR(x_range) |&gt; round(digits = 2)\n  \n  # Display the result\n  return(c(\n    `Mean (SD)` = paste0(mean_val, \" (\", std_val, \")\"),\n    `Median (IQR)` = paste0(med_val, \" (\", iqr_val, \")\")\n  ))\n}\n\nNow, we have the function ready. Let’s test out on the iris dataset.\n\nsummary_func(iris$Sepal.Length)\n\n    Mean (SD)  Median (IQR) \n\"5.84 (0.83)\"   \"5.8 (1.3)\" \n\n\nPerfect! Now, every time we want to calculate the mean, SD, median, and IQR for any column, we can just call our function.\nTo be more efficient we can combine our function with loop or apply family functions that we have learnt in the previous sections. Instead of running summary_func() one by one for each column in the iris dataset, we integrate it in the for loop.\n\n# Loop through numeric columns of the iris dataset\nfor (col in names(iris)[1:4]) {\n  \n    # Calculate mean of the current column\n    res &lt;- summary_func(iris[[col]])\n    \n    # Print the result\n    print(res)\n}\n\n    Mean (SD)  Median (IQR) \n\"5.84 (0.83)\"   \"5.8 (1.3)\" \n    Mean (SD)  Median (IQR) \n\"3.06 (0.44)\"     \"3 (0.5)\" \n    Mean (SD)  Median (IQR) \n\"3.76 (1.77)\"  \"4.35 (3.5)\" \n   Mean (SD) Median (IQR) \n\"1.2 (0.76)\"  \"1.3 (1.5)\" \n\n\nIn R, it is more efficient to use the apply family functions. Let’s use sapply() for this.\n\nsapply(iris |&gt; select(-Species), FUN = summary_func)\n\n             Sepal.Length  Sepal.Width   Petal.Length  Petal.Width \nMean (SD)    \"5.84 (0.83)\" \"3.06 (0.44)\" \"3.76 (1.77)\" \"1.2 (0.76)\"\nMedian (IQR) \"5.8 (1.3)\"   \"3 (0.5)\"     \"4.35 (3.5)\"  \"1.3 (1.5)\" \n\n\nWhat we have learned just now, is known as named function. It is probably the most commonly used function.\n\n7.4.1 Anonymous function\nThere is another type of function in R, known as the anonymous function. It is defined without a name and is often used temporarily.\nFor example, let’s create an anonymous function that squares a number.\n\nfunction(x) { x^2 }\n\nSo, to use this function, we need to type the whole function again. In contrast, the named function can be called by the name of the function to be reused.\n\nfunction(x) { x^2 }(3)\n\nAn example of a practical use of the anonymous function is to be used in line with apply family functions. For example, if we want to square the range of numeric values.\n\nsapply(1:10, function(x) x^2)\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nAnother example of using anonymous functions with the apply family is to generate multiple plots, similar to the example in the for loop section. In this case, we will use walk(), which is the equivalent of the apply family function available in purrr package. Let’s create a boxplot for each numeric column in the iris dataset using walk() and anonymous function.\n\n# Select numeric columns only from iris\ndf &lt;- iris[, -5]\n\n# Create boxplots for each numeric column using walk\nwalk(names(df), function(col_name) {\n  boxplot(df[[col_name]], \n          main = paste(\"Boxplot of\", col_name),\n          ylab = col_name,\n          col = \"skyblue\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function function(col_name) defines an anonymous function to process each column name in names(df). Using walk() allows us to generate multiple boxplots, as it executes the function for its side effects without returning any output. In contrast, using lapply() for this task would return additional information, specifically the statistics for each boxplot, alongside creating the plots.\nTo observe this difference, try running the code below on your machine and compare the behaviour of walk() and lapply() in this context.\n\n# Select numeric columns only from iris\ndf &lt;- iris[, -5]\n\n# Create boxplots for each numeric column using lapply\nlapply(names(df), function(col_name) {\n  boxplot(df[[col_name]], \n          main = paste(\"Boxplot of\", col_name),\n          ylab = col_name,\n          col = \"skyblue\")\n})",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html#chapter-summary",
    "href": "efficient-coding.html#chapter-summary",
    "title": "7  Efficient coding",
    "section": "7.5 Chapter summary",
    "text": "7.5 Chapter summary\nWe have covered adequately about the loop, apply family, and function.\n\n\n\nTable 7.1: Brief summary of loop, apply family, and function.\n\n\n\n\n\n\n\n\n\n\nConcept\nBrief Description\nWhen to Use\n\n\n\n\nLoop\nIterative control structures (e.g., for, while) used to repeat a block of code for a set number of iterations or conditions.\nWhen flexibility and customization are needed for tasks that may not easily fit into vectorized operations.\n\n\nApply Family\nA group of vectorized functions (apply, lapply, sapply, mapply, etc.) that simplify applying functions to data structures.\nWhen performing repetitive operations over data (e.g., rows, columns, or lists) without writing explicit loops.\n\n\nFunction\nA reusable block of code that performs a specific task. Functions can be named or anonymous (e.g., function(x) x^2).\nUse when you need modular, repeatable tasks or computations that are applied across datasets.\n\n\n\n\n\n\nFor readers seeking a greater challenge, I recommend reading the second edition of R for Data Science book, particularly Chapter 25 on functions and Chapter 27 on loops and the apply family (Wickham, Çetinkaya-Rundel, and Grolemund 2023). Additionally, I suggest exploring Chapter 21 of the first edition of R for Data Science, as it covers the loops and apply family more extensively (Wickham and Grolemund 2017).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "efficient-coding.html#revision",
    "href": "efficient-coding.html#revision",
    "title": "7  Efficient coding",
    "section": "7.6 Revision",
    "text": "7.6 Revision\n\nUsing USJudgeRatings dataset, a built-in dataset in R, write a for loop that calculates the median for the first two columns and prints the result. Ensure the medians are rounded to two decimal places.\n\ndata(\"USJudgeRatings\")\n\nWrite a for loop that generates histograms for each numeric column in the iris dataset. Customize each histogram to include a title and an appropriate x-axis label.\nAnalyse the following code and explain why it does not produce the expected output. Correct the code to ensure it calculates the sum of the first 5 rows of the Sepal.Width column in the iris dataset:\n\ncount &lt;- 0\nfor (i in 1:5) {\n    count = count + iris$Sepal.Width\n}\nprint(count)\n\nReview the following code and identify the issue:\n\napply(iris, MARGIN = 1, FUN = mean)\n\nWhy does this produce an error or unexpected output? Correct the code so it calculates the mean of all numeric columns for each row in the iris dataset.\nRewrite the following base R code using the equivalent purrr function:\n\n# Load the library\nlibrary(dplyr)\n\n# Change the codes below to the equivalent purrr function\nlapply(iris %&gt;% select(-Species), mean)\n\nWhat are the difference between lapply() and sapply().\nUsing the summary_func() function in Section 7.4, calculate the statistical summaries (mean, SD, median, IQR) for all numeric columns in the USJudgeRatings dataset, a built-in dataset in R. Use the sapply() function and for loop implementation.\n\ndata(\"USJudgeRatings\")\n\nCreate a function called multiply_num that takes two arguments and returns their product. Use this function to multiply 10 and 20. The function should result an output as below.\n\nmultiply_num(10, 20)\n\n[1] 200\n\n\nModify the avg_num() function in Section 7.4 to include an optional argument round() that rounds the mean to a specified number of decimal places. Test it with the iris$Sepal.Length column, rounding to 3 decimal places. The function should result an output as below.\n\navg_num(iris$Sepal.Length)\n\n[1] 5.843\n\n\nWrite an anonymous function that takes a numeric vector and returns a vector of squared values only for numbers greater than 5. Test this using sapply() with a sequence from 1 to 10.\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media, Inc.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Efficient coding</span>"
    ]
  },
  {
    "objectID": "data-explore.html",
    "href": "data-explore.html",
    "title": "8  Data exploration",
    "section": "",
    "text": "8.1 Missing data\nMissing data are recognised as NA in R. Let’s use airquality data, a built-in dataset in R to see how R recognised missing values.\n# Data with missing data\ndata(\"airquality\")\nBy using summary(), we can see that the first two columns have missing values, recognised as NA.\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0\nAnother function to give a quick answer to whether we have available data or not is anyNA().\nanyNA(airquality)\n\n[1] TRUE\nTo get the count of missing values, we can use is.na():\nis.na(airquality) |&gt; table()\n\n\nFALSE  TRUE \n  874    44\nTRUE is 44, which means that we have 44 missing values across individual cells in our dataset. To drop the missing values, we can use complete.cases():\n# Make an index\nna_index &lt;- complete.cases(airquality)\n\n# Apply the index to drop the NA's\nairquality_noNA &lt;- airquality[na_index, ]\nAlternatively, it is easier to use na.omit():\nairquality_noNA2 &lt;- na.omit(airquality)\nBoth, complete.cases() and na.omit() return an identical output.\n# Drop NA's using complete.cases()\ndim(airquality_noNA)\n\n[1] 111   6\n\n# Drop NA's using na.omit()\ndim(airquality_noNA2)\n\n[1] 111   6\nWe may want to explore the data with missing values. Thus, we can use complete.cases() plus ! in our codes to isolate the data with missing values.\n# Make an index\nna_index &lt;- complete.cases(airquality)\n\n# Apply the index to drop the NA's\nairquality_NA_only &lt;- airquality[!na_index, ]\n\n# Data with missing values\ndim(airquality_NA_only)\n\n[1] 42  6\nNotice that the airquality_NA_only returns 44 rows of data with missing values, while using is.na() earlier we get 44 missing values. airquality_NA_only returns rows of data with missing values. The missing values may exist in more than a single column in the dataset. The is.na() function counts all the missing values regardless of the position.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "data-explore.html#outliers",
    "href": "data-explore.html#outliers",
    "title": "8  Data exploration",
    "section": "8.2 Outliers",
    "text": "8.2 Outliers\nAn outlier is a data point that lies significantly outside the range of most other observations in a dataset. It is an extreme value, either unusually high or low, that differs markedly from other data points. Outliers can occur due to variability in the data, errors in measurement, or experimental anomalies.\nSince outliers may not be representative of the data and may distort statistical measures such as mean and standard deviation, it is important to identify them early during the data exploration stage.\nThere are two methods to identify outliers:\n\nInterquartile range (IQR) method:\nThe IQR method identifies outliers as data points lying below Q1 − 1.5 × IQR or above Q3 + 1.5 × IQR , where Q1 and Q3 are the first and third quartiles, respectively. This is the method that applies in the boxplot.\nLet’s use airquality_noNA data to demonstrate:\n\nout_bp &lt;- boxplot(airquality_noNA$Wind, main = \"Outliers in the wind column\")\n\n\n\n\n\n\n\n\nTo access the values of the outliers:\n\nout_bp$out\n\n[1] 20.1 18.4 20.7\n\n\nTo see the rows with the outliers:\n\nairquality_noNA |&gt; \n  dplyr::filter(Wind == c(20.1, 18.4, 20.7))\n\n  Ozone Solar.R Wind Temp Month Day\n1     8      19 20.1   61     5   9\n2     6      78 18.4   57     5  18\n3    37     284 20.7   72     6  17\n\n\nZ-score method:\nThis method standardizes data and identifies outliers as those with a z-score greater than a threshold (commonly 3 or -3). The z-score (also called a standard score) is a statistical measure that indicates how many standard deviations a data point is from the mean of the dataset.\nBasically, the z-score is a distribution which reflects our dataset and it shows how far each value is from the average. So, in this context, outliers are the values that are significantly far from the average.\nNow, let’s focus on understanding how we can detect the outliers using this method.\n\n# Create z-score for the data\nz_scores &lt;- scale(airquality_noNA$Wind)\n\n# How many outliers\ntable(abs(z_scores) &gt; 3)\n\n\nFALSE  TRUE \n  110     1 \n\n\nTRUE is 1, which means that we have a single outlier in our data. Let’s see which value is the outlier.\n\nairquality_noNA$Wind[abs(z_scores) &gt; 3]\n\n[1] 20.7\n\n\nTo see the row with the outlier:\n\nairquality_noNA |&gt; \n  dplyr::filter(Wind == 20.7)\n\n  Ozone Solar.R Wind Temp Month Day\n1    37     284 20.7   72     6  17\n\n\n\nThere are other advanced methods to detect outliers. However, the basics to understand those methods are beyond the scope of this book.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "data-explore.html#sec-data-explore-useful-packages",
    "href": "data-explore.html#sec-data-explore-useful-packages",
    "title": "8  Data exploration",
    "section": "8.3 Useful packages",
    "text": "8.3 Useful packages\nIn this section, we going to see several useful R packages to explore the data. We are only going to cover the main functions of each package.\n\n8.3.1 skimr\nThe skimr package provides various helpful functions to do data exploration. Firstly, we need to install skimr, and then load the packages. We going to use the dplyr package together with the skimr package.\n\n# Install the package\ninstall.packages(\"skimr\")\n\n# Load the necessary packages\nlibrary(skimr)\nlibrary(dplyr)\n\nTo explore the whole dataset, we can use skim.\n\n# Let's use the iris dataset\ndata(\"iris\")\n\n# Use skim\nskim(iris)\n\n\nData summary\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\nThe skim function will return the basic statistics for our data including the information on the missing values (n_missing and complete_rate) and a histogram for the numerical variables. Additionally, we can get the basic statistics based on a certain group. For example, here we use group_by(), then, we apply skim().\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nSpecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nSpecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\nsetosa\n0\n1\n5.01\n0.35\n4.3\n4.80\n5.00\n5.20\n5.8\n▃▃▇▅▁\n\n\nSepal.Length\nversicolor\n0\n1\n5.94\n0.52\n4.9\n5.60\n5.90\n6.30\n7.0\n▂▇▆▃▃\n\n\nSepal.Length\nvirginica\n0\n1\n6.59\n0.64\n4.9\n6.23\n6.50\n6.90\n7.9\n▁▃▇▃▂\n\n\nSepal.Width\nsetosa\n0\n1\n3.43\n0.38\n2.3\n3.20\n3.40\n3.68\n4.4\n▁▃▇▅▂\n\n\nSepal.Width\nversicolor\n0\n1\n2.77\n0.31\n2.0\n2.52\n2.80\n3.00\n3.4\n▁▅▆▇▂\n\n\nSepal.Width\nvirginica\n0\n1\n2.97\n0.32\n2.2\n2.80\n3.00\n3.18\n3.8\n▂▆▇▅▁\n\n\nPetal.Length\nsetosa\n0\n1\n1.46\n0.17\n1.0\n1.40\n1.50\n1.58\n1.9\n▁▃▇▃▁\n\n\nPetal.Length\nversicolor\n0\n1\n4.26\n0.47\n3.0\n4.00\n4.35\n4.60\n5.1\n▂▂▇▇▆\n\n\nPetal.Length\nvirginica\n0\n1\n5.55\n0.55\n4.5\n5.10\n5.55\n5.88\n6.9\n▃▇▇▃▂\n\n\nPetal.Width\nsetosa\n0\n1\n0.25\n0.11\n0.1\n0.20\n0.20\n0.30\n0.6\n▇▂▂▁▁\n\n\nPetal.Width\nversicolor\n0\n1\n1.33\n0.20\n1.0\n1.20\n1.30\n1.50\n1.8\n▅▇▃▆▁\n\n\nPetal.Width\nvirginica\n0\n1\n2.03\n0.27\n1.4\n1.80\n2.00\n2.30\n2.5\n▂▇▆▅▇\n\n\n\n\n\nReaders interested in learning more about the skimr package can explore its comprehensive documentation for more details and examples.\n\n\n8.3.2 naniar\nnaniar package provides tidy ways to summarise, visualise, and manipulate missing data. First, let’s install and load the necessary packages.\n\n# Install the package\ninstall.packages(\"naniar\")\n\n# Load the necessary packages\nlibrary(naniar)\nlibrary(dplyr)\n\nLet’s use the oceanbuoys data, a dataset from the naniar package. First, let’s summarise the missing data according to a variable. Further detail on the dataset can be found by running ?oceanbuoys in the Console.\n\n# Load the data\ndata(\"oceanbuoys\")\n\n# Missing values summary based on variables\nmiss_var_summary(oceanbuoys)\n\n# A tibble: 8 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 humidity       93   12.6  \n2 air_temp_c     81   11.0  \n3 sea_temp_c      3    0.408\n4 year            0    0    \n5 latitude        0    0    \n6 longitude       0    0    \n7 wind_ew         0    0    \n8 wind_ns         0    0    \n\n\nn_miss is the number of missing values and pct_miss is the percentage of missing values. We can further group the missing values according to a certain variable.\n\noceanbuoys %&gt;% \n  group_by(year) %&gt;% \n  miss_var_summary()\n\n# A tibble: 14 × 4\n# Groups:   year [2]\n    year variable   n_miss pct_miss\n   &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n 1  1997 air_temp_c     77   20.9  \n 2  1997 latitude        0    0    \n 3  1997 longitude       0    0    \n 4  1997 sea_temp_c      0    0    \n 5  1997 humidity        0    0    \n 6  1997 wind_ew         0    0    \n 7  1997 wind_ns         0    0    \n 8  1993 humidity       93   25.3  \n 9  1993 air_temp_c      4    1.09 \n10  1993 sea_temp_c      3    0.815\n11  1993 latitude        0    0    \n12  1993 longitude       0    0    \n13  1993 wind_ew         0    0    \n14  1993 wind_ns         0    0    \n\n\nnaniar also provides a visual summary for missing values.\n\ngg_miss_var(oceanbuoys)\n\n\n\n\n\n\n\n\nWe can further group by a variable using the facet argument.\n\ngg_miss_var(oceanbuoys, facet = year)\n\n\n\n\n\n\n\n\nThe content we just covered provides only a glimpse of the powerful features and capabilities of the naniar package. For a deeper understanding and comprehensive insights, readers are encouraged to explore the naniar documentation website.\n\n\n8.3.3 DataExplorer\nDataExplorer provides various helpful functions for data exploration. First, let’s install and load the necessary packages.\n\n# Install package\ninstall.packages(\"DataExplorer\")\n\n# Load the necessary packages\nlibrary(DataExplorer)\nlibrary(dplyr)\n\nLet’s use oceanbuoys data from the naniar package previously.\n\n# Load the data \ndata(\"oceanbuoys\", package = \"naniar\")\n\nDataExplorer provides a general function to explore the data.\n\nplot_intro(oceanbuoys)\n\n\n\n\n\n\n\n\nFrom the plot, we understand that our data consists of all continuous (numeric) columns and no discrete (categorical) columns. No missing columns but we have 3% missing observations.\nTo investigate the missing observations, we can use plot_missing():\n\nplot_missing(oceanbuoys)\n\n\n\n\n\n\n\n\nAlternatively, we can get a summary instead of a plot.\n\nprofile_missing(oceanbuoys)\n\n# A tibble: 8 × 3\n  feature    num_missing pct_missing\n  &lt;fct&gt;            &lt;int&gt;       &lt;dbl&gt;\n1 year                 0     0      \n2 latitude             0     0      \n3 longitude            0     0      \n4 sea_temp_c           3     0.00408\n5 air_temp_c          81     0.110  \n6 humidity            93     0.126  \n7 wind_ew              0     0      \n8 wind_ns              0     0      \n\n\nAdditionally, DataExplorer provides a function to plot a correlation matrix. Correlation is a measure of association between two numerical variables. It ranges between -1 and 1. Values close to 1 indicate a high positive correlation between the two variables, while values close to -1 indicate a high negative correlation between the two numerical variables. Values close to 0 indicate a low correlation between the two values.\n\noceanbuoys %&gt;% \n  na.omit() %&gt;% \n  plot_correlation()\n\n\n\n\n\n\n\n\nTo do a correlation or in this case correlation plot, the variables should be numerical and have no missing values (hence, the use of na.omit() in the code). If for example, we want to apply this function to the iris dataset, in which we know that one of the variables is categorical, we need to exclude the variable first.\n\niris %&gt;% \n  select(-Species) %&gt;% \n  plot_correlation()\n\n\n\n\n\n\n\n\nAlternatively, the more efficient code to exclude the non-numerical variable is using the select_if().\n\niris %&gt;% \n  select_if(is.numeric) %&gt;% \n  plot_correlation()\n\n\n\n\n\n\n\n\nDataExplorer provides more useful functions, which can not be extensively covered in this section. Interested readers can further study its documentation for more.\n\n\n8.3.4 VIM\nThe VIM package contains tools for the visualisation of missing and/or imputed values. Imputation of the missing values is beyond the scope of this book. However, we going to see how functions from the VIM package can be utilised to explore the pattern of missingness.\nFirst, make sure to install the VIM package and load the necessary packages.\n\n# Install package\ninstall.packages(\"VIM\")\n\n# Load the necessary packages\nlibrary(VIM)\nlibrary(dplyr)\n\nLet’s use oceanbuoys data from the naniar package previously.\n\n# Load the data \ndata(\"oceanbuoys\", package = \"naniar\")\n\nThe aggr() function will plot our data and visualise the pattern of missing values. numbers = TRUE and prop = FALSE to make sure the number missing values in a number not a proportion.\n\naggr(oceanbuoys, numbers = TRUE, prop = FALSE)\n\n\n\n\n\n\n\n\nThe red colour represents the missing values, and the blue colour represents the observed values. The first plot on the left presents the proportion of missing values according to variables. The second plot on the right presents the combination of missing values. Notice that if we have too many variables, the variable names will not be fully displayed. For example in the second plot, for a combination of sea_temp_c, air_temp_c, and humidity, we have 2 values missing.\nThe aggr() function is suitable to be utilised when we have a small to intermediate number of variables. For data with a large number of variables, matrixplot() may be more appropriate.\n\nmatrixplot(oceanbuoys)\n\n\n\n\n\n\n\n\nmatrixplot() scales the data into 0 (white) and 1 (black). The higher values will become close to 1, and the lower values will become close to 0. The red colour represents the missing values.\nThe VIM package contains more useful functions, however, the majority of them are for exploring the imputation methods and values. Interested readers can further study its documentation for more details.\n\n\n8.3.5 dlookr\nThe dlookr package provides various helpful functions especially related to outliers. First, we need to install the dlookr package and load the necessary packages.\n\n# Install package\ninstall.packages(\"dlookr\")\n\n# Load the packages\nlibrary(dlookr)\nlibrary(dplyr)\n\nLet’s use the Carseats dataset, a dataset from dlookr package. The diagnose_numeric() function from the dlookr package is particularly useful for diagnosing numeric variables.\n\n# Load the data\ndata(\"Carseats\")\n\n# Diagnose function\ndiagnose_numeric(Carseats)\n\n    variables min     Q1       mean median     Q3    max zero minus outlier\n1       Sales   0   5.39   7.496325   7.49   9.32  16.27    1     0       2\n2   CompPrice  77 115.00 124.975000 125.00 135.00 175.00    0     0       2\n3      Income  21  42.75  68.657500  69.00  91.00 120.00    0     0       0\n4 Advertising   0   0.00   6.635000   5.00  12.00  29.00  144     0       0\n5  Population  10 139.00 264.840000 272.00 398.50 509.00    0     0       0\n6       Price  24 100.00 115.795000 117.00 131.00 191.00    0     0       5\n7         Age  25  39.75  53.322500  54.50  66.00  80.00    0     0       0\n8   Education  10  12.00  13.900000  14.00  16.00  18.00    0     0       0\n\n\nAmong its results, it provides the following insights:\n\nzero: the number of zero values in the data.\nminus: the count of negative values.\noutlier: the number of potential outliers detected in the dataset.\n\nAdditionally, we have diagnose_category() which summarises categorical variables. This function returns several outputs:\n\nlevels: level for each categorical variable.\nN: number of observations.\nfreq: number of observations at the levels.\nratio: percentage of observations at the levels\nrank: rank of occupancy ratio of levels\n\n\ndiagnose_category(Carseats)\n\n  variables levels   N freq ratio rank\n1 ShelveLoc Medium 400  219 54.75    1\n2 ShelveLoc    Bad 400   96 24.00    2\n3 ShelveLoc   Good 400   85 21.25    3\n4     Urban    Yes 400  282 70.50    1\n5     Urban     No 400  118 29.50    2\n6        US    Yes 400  258 64.50    1\n7        US     No 400  142 35.50    2\n\n\nWe can further explore the outliers identified by diagnose_numeric() using diagnose_outlier().\n\ndiagnose_outlier(Carseats)\n\n    variables outliers_cnt outliers_ratio outliers_mean  with_mean without_mean\n1       Sales            2           0.50         15.95   7.496325     7.453844\n2   CompPrice            2           0.50        126.00 124.975000   124.969849\n3      Income            0           0.00           NaN  68.657500    68.657500\n4 Advertising            0           0.00           NaN   6.635000     6.635000\n5  Population            0           0.00           NaN 264.840000   264.840000\n6       Price            5           1.25        100.40 115.795000   115.989873\n7         Age            0           0.00           NaN  53.322500    53.322500\n8   Education            0           0.00           NaN  13.900000    13.900000\n\n\nThis function returns:\n\noutliers_cnt: number of outliers.\noutliers_ratio: percent of outliers.\noutliers_mean: mean of outliers.\nwith_mean: mean of values with outliers.\nwithout_mean: mean of values of without outliers.\n\nIn fact, we can further simplify the result using the filter() from the dplyr package.\n\ndiagnose_outlier(Carseats) %&gt;% \n  filter(outliers_cnt &gt; 0)\n\n  variables outliers_cnt outliers_ratio outliers_mean  with_mean without_mean\n1     Sales            2           0.50         15.95   7.496325     7.453844\n2 CompPrice            2           0.50        126.00 124.975000   124.969849\n3     Price            5           1.25        100.40 115.795000   115.989873\n\n\nFurthermore, dlookr provides another useful function to visualise the outliers. However, for this example, we going to select a single variable.\n\nCarseats %&gt;% \n  select(Sales) %&gt;% \n  plot_outlier()\n\n\n\n\n\n\n\n\nThis function returns a boxplot and histogram for values with and without outliers. Thus, we can see how much the outliers in the variables change the distribution of the data. dlookr actually provides more functions than the ones we covered in this section. Readers are suggested to go through its documentation to further learn this package.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "data-explore.html#chapter-summary",
    "href": "data-explore.html#chapter-summary",
    "title": "8  Data exploration",
    "section": "8.4 Chapter summary",
    "text": "8.4 Chapter summary\nIn this chapter, we have covered how missing data and outliers were identified in R using base R functions. Moreover, we have covered several useful functions from five R packages:\n\nskimr\nnaniar\nDataExplorer\nVIM\ndlookr\n\nskimr provides general functions for data exploration (Waring et al. 2024). naniar, DataExplorer, and VIM provide additional functions to explore and investigate missing data (Tierney and Cook 2023; Cui 2024; Kowarik and Templ 2016). Lastly, dlookr provides more functions for investigating outliers in the dataset (Ryu 2024).\nWhile this chapter highlights the capabilities of these five packages, it is important to note that R offers many additional packages that can further enhance your analysis. Readers are encouraged to explore beyond these tools to find packages best suited to their specific needs.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "data-explore.html#revision",
    "href": "data-explore.html#revision",
    "title": "8  Data exploration",
    "section": "8.5 Revision",
    "text": "8.5 Revision\n\nWhy is data exploration considered a crucial first step in data analysis, and what are some R functions and packages that can help in this process?\nWrite an R code to check for missing values in each column of the riskfactors dataset, a dataset from the naniar package. Then, count how many NA values exist in total.\n\n# Load the data\ndata(\"riskfactors\", package = \"naniar\")\n\nUsing the riskfactors dataset from question 2, write R code to remove all missing values in the dataset and determine how many rows are left.\nBesides dropping the missing values, what are the possible solutions to missing values?\nWhat are the possible solutions for outliers?\nWhat are the pros and cons of dropping:\n\nMissing values\nOutliers\n\nName three packages for data exploration besides the five packages covered in this chapter.\n\n\n\n\n\nCui, Boxuan. 2024. DataExplorer: Automate Data Exploration and Treatment. http://boxuancui.github.io/DataExplorer/.\n\n\nKowarik, Alexander, and Matthias Templ. 2016. “Imputation with the r Package VIM.” Journal of Statistical Software 74 (7): 1–16. https://doi.org/10.18637/jss.v074.i07.\n\n\nRyu, Choonghyun. 2024. Dlookr: Tools for Data Diagnosis, Exploration, Transformation. https://CRAN.R-project.org/package=dlookr.\n\n\nTierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” Journal of Statistical Software 105 (7): 1–31. https://doi.org/10.18637/jss.v105.i07.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2024. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html",
    "href": "descriptive-stat.html",
    "title": "9  Descriptive statistics",
    "section": "",
    "text": "9.1 Load packages\nPlease load these packages before proceeding to the next section.\n# Install the packages if necessary\ninstall.packages(\"tidyverse\")\ninstall.packages(\"DescTools\")\ninstall.packages(\"summarytools\")\ninstall.packages(\"gtsummary\")\n\n# Load the packages\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html#measures-of-central-tendency",
    "href": "descriptive-stat.html#measures-of-central-tendency",
    "title": "9  Descriptive statistics",
    "section": "9.2 Measures of central tendency",
    "text": "9.2 Measures of central tendency\nMeasures of central tendency aim to identify the centre or typical value within a dataset. It provides a summary of the data by describing the point around which most values cluster.\nMean\nThe mean represents the average of a dataset. It is calculated by summing all the values in the dataset and dividing the total by the number of values. \\[\nMean(\\overline{x}) = \\frac{\\sum x}{n}\n\\]\nFor example, to calculate the mean manually for the numbers:\n\\[\n5, 6, 7, 8, 13, 2\n\\]\n\nSum up all the values (\\(\\Sigma x\\)).\n\\[\n5 + 6 + 7 + 8 + 13 + 2 = 41\n\\]\nDivide the sum of all the values by the count of the values (\\(n\\)).\n\\[\n\\frac{41}{6} = 6.83\n\\]\n\nAlternatively, R provides a mean() function to calculate the mean of values.\n\n# The values\nx_mean = c(5, 6, 7, 8, 13, 2)\n\n# Calculate mean\nmean(x_mean)\n\n[1] 6.833333\n\n\nMedian\nThe median represents the middle value in a sorted dataset. It divides the dataset into two halves: 50% of the data values are smaller than the median, and 50% are larger.\nFor example, to determine the median for the below values:\n\\[\n5, 6, 7, 8, 13, 2\n\\]\n\nSort the values in order from the smallest to the largest.\n\\[\n2, 5, 6, 7, 8, 13\n\\]\nDetermine the middle values (if there are two middle values, calculate the average of the two values).\n\\[\n\\frac{6 + 7}{2} = 6.5\n\\]\n\nAlternatively, to calculate the median in R, we can use the median() function.\n\n# The values\nx_med = c(5, 6, 7, 8, 13, 2)\n\n# Calculate the median\nmedian(x_med)\n\n[1] 6.5\n\n\nMode\nThe mode represents the most frequently occurring value in a dataset. It is the value that appears the most times. Unlike the mean or median, the mode can be used for both numerical and categorical data.\nFor example, to determine the mode for the numbers:\n\\[\n5, 6, 7, 8, 13, 2, 8, 1\n\\]\nThe mode is the most frequent value which is 8.\nIn R, there is no function in base R to determine the mode. We can manually determine the mode in R.\n\n# Range of values\nx_mode_num = c(5, 6, 7, 8, 13, 2, 8, 1)\n\n# Calculate the count for each value\nx_mode_num_table &lt;- \n  x_mode_num %&gt;% \n  table() %&gt;% \n  as.data.frame()\nx_mode_num_table\n\n   . Freq\n1  1    1\n2  2    1\n3  5    1\n4  6    1\n5  7    1\n6  8    2\n7 13    1\n\n# Determine the frequently appearing value\nx_mode_num_table %&gt;% \n  filter(Freq == max(Freq))\n\n  . Freq\n1 8    2\n\n\nAdditionally, we can determine the mode for the categorical data using the same approach:\n\n# Range of values\nx_mode_cat &lt;- c(\"yes\", \"no\", \"unsure\", \"yes\", \"yes\", \"no\")\n\n# Calculate the count for each value\nx_mode_cat_table &lt;- \n  x_mode_cat %&gt;% \n  table() %&gt;% \n  as.data.frame()\nx_mode_cat_table\n\n       . Freq\n1     no    2\n2 unsure    1\n3    yes    3\n\n# Determine the frequently appearing value\nx_mode_cat_table %&gt;% \n  filter(Freq == max(Freq))\n\n    . Freq\n1 yes    3\n\n\nAlternatively, there are several packages available to determine the mode. For example, we can use the DescTools package.\n\n# Determine the mode for numerical values\nDescTools::Mode(x_mode_num)\n\n[1] 8\nattr(,\"freq\")\n[1] 2\n\n# Determine the mode for categorical values\nDescTools::Mode(x_mode_cat)\n\n[1] \"yes\"\nattr(,\"freq\")\n[1] 3\n\n\nThe Mode() function from DescTools returns the mode value and the frequency of the values. For example, yes is the mode and it appears in the data 3 times.\n\n9.2.1 Comparison of mean, median, and mode\nMean vs. median\nThe mean is sensitive to outliers compared to the median and mode. In the presence of outliers, the median is more robust and thus, preferred to describe the central tendency of the data. Let’s demonstrate the change of mean and median in the presence of the outliers.\nFirstly, let’s create the data with and without the outlier.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\noutlier_data &lt;- data.frame(\n  outlier = c(sample(x = c(1:10), size = 20, replace = TRUE), 100),\n  normal = sample(x = c(1:10), size = 21, replace = TRUE)\n)\n\nBefore we demonstrate further, let’s change the data structure, so, that it is easier to do the plots.\n\noutlier_data_long &lt;- \n  outlier_data %&gt;% \n  # Change data structure to long data\n  pivot_longer(cols = 1:2, names_to = \"Variable\", values_to = \"Value\")\n\nLet’s plot the boxplot for both data to visualise the outlier.\n\noutlier_data_long %&gt;% \n  ggplot(aes(x = Value, fill = Variable)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_bw() +\n  labs(\n    title = \"Boxplot of variables with and without the outlier\",\n    x = \"Value\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nWe can see the outlier in the var_outlier. If we plot the histogram for both variables, we can further visualise the distribution of the data.\n\noutlier_data_long %&gt;% \n  ggplot(aes(x = Value, fill = Variable)) +\n  geom_histogram(alpha = 0.6, bins = 50) +\n  facet_grid(rows = vars(Variable)) +\n  theme_bw() +\n  labs(\n    title = \"Histogram of variables with and without the outlier\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIn the histogram, it is evident that the majority of values are concentrated between 0 and 25. When comparing the mean and median for both variables, with and without the outlier, the median demonstrates greater stability irrespective of the outlier’s presence. The medians for both variables remain relatively similar, whereas the means show significant variation, reflecting the mean’s sensitivity to outliers.\n\nsummary_stats &lt;- \n  outlier_data_long %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    mean = mean(Value),\n    median = median(Value)\n  )\nsummary_stats\n\n# A tibble: 2 × 3\n  Variable  mean median\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 normal    6.19      7\n2 outlier  11         7\n\n\nLet’s put the values of the mean and median onto a histogram so that we can see how these values change in the presence of the outlier.\n\noutlier_data_long %&gt;% \n  ggplot(aes(x = Value, fill = Variable)) +\n  geom_histogram(alpha = 0.6, bins = 50) +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = mean, color = Variable), \n    linetype = \"dashed\", \n    linewidth = 0.8\n    ) +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = median, color = Variable), \n    linetype = \"solid\", \n    linewidth = 0.8\n    ) +\n  scale_color_manual(values = c(\"black\", \"black\")) +\n  labs(\n    title = \"Histograms of outlier data with mean and median\",\n    x = \"Value\",\n    y = \"Count\",\n    subtitle = \"(Dashed line = mean, solid line = median)\"\n  ) +\n  facet_grid(rows = vars(Variable)) +\n  theme_bw() +    \n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_text(face = \"italic\")\n    ) \n\n\n\n\n\n\n\n\nFrom the histograms, we can see the median (solid line) is relatively at the same position while the mean (the dashed line) moves according to the outlier.\nBesides the sensitivity of the mean to the outliers, the mean is the best measure of central tendency for normally distributed data because it incorporates all data points and accurately reflects the centre of a symmetric distribution. In a normal distribution, data points are symmetrically distributed around the mean, and thus, the mean provides a reliable measure of the “typical” value. However, it is to be noted despite the mean being more preferred in normal distribution, the values of median and mode are very close to the mean.\nOn the other hand, for the skewed distribution, the median is preferred because it is not influenced by extreme values (outliers) that can distort the mean. In a skewed distribution, where data points are unevenly spread, the median represents the middle value, providing a better reflection of the “typical” data point compared to the mean. This makes the median more robust when dealing with outliers and skewed data, as the mean may be pulled toward the skewed tail, giving a misleading representation of central tendency.\nAgain, let’s demonstrate this point in R. First, we create the data consisting of two variables of normal and skewed distributions.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\ndistribution_data &lt;- \n  data.frame(\n    normal = rnorm(500, mean = 0, sd = 1),\n    # Create a skewed distribution variable (an exponential distribution with rate = 1)\n    skewed = rexp(500, rate = 1))\n\nNext, we need to change the data structure to a long data type to fully explore the data.\n\ndistribution_data_long &lt;- \n  distribution_data %&gt;% \n  pivot_longer(cols = 1:2, names_to = \"distribution\", values_to = \"values\")\n\nWe going to plot the histogram to visualise the distributions.\n\ndistribution_data_long %&gt;% \n  ggplot(aes(x = values, fill = distribution)) +\n  geom_histogram(bins = 30, alpha = 0.6) +\n  facet_grid(rows = vars(distribution)) +\n  labs(title = \"Histogram of normal and skewed distribution data\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNext, let’s calculate the mean and median for each of the distributions.\n\nsummary_stat_dist &lt;- \n  distribution_data_long %&gt;% \n  group_by(distribution) %&gt;% \n  summarise(\n    mean = mean(values),\n    median = median(values)\n    )\nsummary_stat_dist\n\n# A tibble: 2 × 3\n  distribution   mean median\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 normal       0.0346 0.0207\n2 skewed       1.05   0.727 \n\n\nNotice that the mean and median for normal distribution are close while skewed distribution is otherwise. Let’s visualise the mean and median in the histogram.\n\ndistribution_data_long %&gt;% \n  ggplot(aes(x = values, fill = distribution)) +\n  geom_histogram(alpha = 0.6, bins = 50) +\n  geom_vline(\n    data = summary_stat_dist, \n    aes(xintercept = mean, color = distribution), \n    linetype = \"dashed\", \n    linewidth = 0.8\n    ) +\n  geom_vline(\n    data = summary_stat_dist, \n    aes(xintercept = median, color = distribution), \n    linetype = \"solid\", \n    linewidth = 0.8\n    ) +\n  scale_color_manual(values = c(\"black\", \"black\")) +\n  labs(\n    title = \"Normal and skewed distributions with their mean and median\",\n    x = \"Value\",\n    y = \"Count\",\n    subtitle = \"(Dashed line = mean, solid line = median)\"\n    ) +\n  facet_grid(rows = vars(distribution)) +\n  theme_bw() +    \n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_text(face = \"italic\")\n    ) \n\n\n\n\n\n\n\n\nIn the normal distribution, despite the values of mean and median being close, the mean is preferred as it takes into account the whole range of values mathematically. However, in the skewed distribution, the median is preferred as it is less affected by the extreme values and sparsity of the values as compared to the mean.\nMode vs. mean and median\nLastly, the mode is less useful for the data analysis compared to the mean and median. Moreover, the mode may not be unique or exist in certain datasets. Let’s demonstrate these cases:\n\nMode values are not unique.\n\n# Values\ntwo_mode_data &lt;- c(\"yes\", \"no\", \"unsure\", \"yes\", \"yes\", \"no\", \"no\")\n\n# Determine the mode\nDescTools::Mode(two_mode_data)\n\n[1] \"no\"  \"yes\"\nattr(,\"freq\")\n[1] 3\n\n\nMode value does not exist.\n\n# Values\nmode_not_exist &lt;- c(\"yes\", \"unsure\", \"no\")\n\n# Determine the mode\nDescTools::Mode(mode_not_exist)\n\n[1] NA\nattr(,\"freq\")\n[1] NA",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html#measures-of-variability",
    "href": "descriptive-stat.html#measures-of-variability",
    "title": "9  Descriptive statistics",
    "section": "9.3 Measures of variability",
    "text": "9.3 Measures of variability\nMeasures of variability describe the spread or dispersion of data points in a dataset. These measures indicate how much the data values differ from each other and from the central tendency (e.g., mean or median).\nRange\nThe range is the difference between maximum and minimum values in a dataset.\n\\[\nRange = Maximum - Minimum\n\\]\nFor example, the range for this range of values:\n\\[\n1, 2, 5, 8, 10, 5\n\\]\n\nIdentify the maximum and minimum values.\n\\[\nMaximum = 10, Minimum = 1\n\\]\nCalculate the range.\n\\[\n10 - 1 = 9\n\\]\n\nIn R, we have a range() function. However, instead of giving a range, the range() function returns a minimum and maximum value.\n\n# The numeric values\nx_range &lt;- c(1, 2, 5, 8, 10, 5)\n\n# Range function\nrange(x_range)\n\n[1]  1 10\n\n\nTo calculate the range in R, we can utilise max() and min() functions, which identify the maximum and minimum values, respectively.\n\nmax(x_range) - min(x_range)\n\n[1] 9\n\n\nVariance\nVariance measures the average squared difference of each data point from the mean. It shows how data points spread around the mean in squared units.\n\\[\nVariance (s^{2}) = \\frac{\\sum (x_{i} - \\overline{x})^{2}}{n-1}\n\\]\nFor example, to calculate the variance for these data points:\n\\[\n3, 5, 6, 8, 2, 9\n\\]\n\nCalculate the mean (\\(\\overline{x}\\)).\n\\[\n\\frac{3 + 5 + 6 + 8 + 2 + 9}{6} = 5.5\n\\]\nCalculate the difference between each data point and mean, square it, and sum up all the squared differences (\\(\\sum (x_{i} - \\overline{x})^{2}\\)).\n\\[\n(3-5.5)^{2}+(5-5.5)^{2}+(6-5.5)^{2}+(8-5.5)^{2}+(2-5.5)^{2}+(9-5.5)^{2} = 37.5\n\\]\nDivide the value by the count of the numbers (\\(n-1\\)).\n\\[\n\\frac{37.5}{6-1} = 7.5\n\\]\n\nIn R, we can calculate the variance using the var() function.\n\n# The values\nx_variance &lt;- c(3, 5, 6, 8, 2, 9)\n\n# Calculate variance\nvar(x_variance)\n\n[1] 7.5\n\n\nThe equation that we used to calculate the variance manually and the one in R is known as sample variance. There is another equation known as population variance. The sample variance is used to describe the spread out of the values in a sample, while population variance describes the spread out of the values in a population.\n\\[\n\\begin{align*}\n\\text{Sample variance}(s^{2}) = \\frac{\\sum (x_{i} - \\overline{x})^{2}}{n-1}\\\\\n\\text{Population variance}(\\sigma^{2})= \\frac{\\sum (x_{i} - \\mu)^{2}}{N}\n\\end{align*}\n\\]\nNotice the slight differences in the denominators of the two equations. Sample variance is calculated for a subset of data and includes an adjustment for smaller sample sizes by dividing by \\(n-1\\) instead of \\(n\\). This correction (known as Bessel’s correction) ensures that the sample variance is an unbiased estimate of the population variance. In contrast, population variance is calculated for the entire dataset and does not require this adjustment. Additionally, instead of using sample mean (\\(\\overline{x}\\)), population variance is using the population mean (\\(\\mu\\)).\nIn practical data analysis, we are almost always working with a sample rather than the entire population. As a result, the formula for sample variance is typically used to estimate the variability of the dataset.\nStandard deviation\nStandard deviation is the square root of the variance, providing a measure of dispersion in the same units as the data. A smaller value indicates less variability and a larger value indicates greater variability.\n\\[\n\\text{Standard deviation}(SD) = \\sqrt{Variance}\n\\]\nFor example, to calculate the standard deviation for these numbers:\n\\[\n3, 5, 6, 8, 2, 9\n\\]\n\nCalculate the sample variance (\\(s^{2}\\)).\n\\[\n\\frac{(3-5.5)^{2}+(5-5.5)^{2}+(6-5.5)^{2}+(8-5.5)^{2}+(2-5.5)^{2}+(9-5.5)^{2}}{6-1} = 7.5\n\\]\nSquare root the sample variance (\\(s^{2}\\)).\n\\[\n\\sqrt{7.5} = 2.74\n\\]\n\nIn R, we can use the sd() function to calculate the standard deviation, This function uses sample variance to calculate the standard deviation.\n\n# The values\nx_sd &lt;- c(3, 5, 6, 8, 2, 9)\n\n# Calculate variance\nsd(x_sd)\n\n[1] 2.738613\n\n\nInterquartile range\nThe interquartile range describes the range of the middle 50% of the data (the difference between the third quartile and first quartile).\n\\[\n\\text{Interquartile range}(IQR) = Q_{3} - Q_{1}\n\\]\nTo calculate the interquartile range for these numbers:\n\\[\n3, 5, 6, 8, 2, 9\n\\]\n\nDivide the data into lower half and upper half\n\\[\n\\begin{align*}\n\\text{Lower half} = 2, 3, 5\\\\\n\\text{Upper half} = 6, 8, 9\n\\end{align*}\n\\]\nIdentify the median of the lower half (first quartile, \\(Q_{1}\\)) and the median of the upper half (third quartile, \\(Q_{3}\\)).\n\\[\n\\begin{align*}\n\\text{Median of lower half}(Q_{1}) = 3\\\\\n\\text{Median of the upper half} (Q_{3}) = 8\n\\end{align*}\n\\]\nCalculate the interquartile range.\n\\[\n8-3 = 5\n\\]\n\nIn R, we can use IQR() to calculate the interquartile range.\n\n# The values\nx_iqr &lt;- c(3, 5, 6, 8, 2, 9)\n\n# Calculate interquartile range\nIQR(x_iqr, type = 2)\n\n[1] 5\n\n\nIQR() actually uses the quantile() function to calculate the quartiles, subsequently calculating the interquartile range. quantile() with type = 2 reflects the exact first and the third quartile that we manually calculate. quantile() and IQR() have 9 types of algorithms or formulas to calculate the quartile in R. The default, both functions use type = 7. However, Hyndman & Fan proposed to use type 8 (1996).\n\n# Quantile type 2 reflects our manual calculation\nquantile(x_iqr, type = 2)\n\n  0%  25%  50%  75% 100% \n 2.0  3.0  5.5  8.0  9.0 \n\n# IQR type 2 reflects our manual calculation\nIQR(x_iqr, type = 2)\n\n[1] 5\n\n# Quantile type 7 is the default\n# We don't actually need to specify the type since it is the default\nquantile(x_iqr, type = 7) \n\n  0%  25%  50%  75% 100% \n 2.0  3.5  5.5  7.5  9.0 \n\n# IQR type 7 is the default\nIQR(x_iqr, type = 7)\n\n[1] 4\n\n# Quantile type 8 \nquantile(x_iqr, type = 8) \n\n      0%      25%      50%      75%     100% \n2.000000 2.916667 5.500000 8.083333 9.000000 \n\n# IQR type 8 \nIQR(x_iqr, type = 8)\n\n[1] 5.166667\n\n\nBriefly, I suggest using type = 7 or type = 8 for calculating the quartile and interquartile range. For accuracy, we can use type = 8 as proposed by Hyndman & Fan(1996). For simplicity, we can use type = 7 as it is the default in R and widely used in other statistical software as well.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html#packages-for-descriptive-statistics",
    "href": "descriptive-stat.html#packages-for-descriptive-statistics",
    "title": "9  Descriptive statistics",
    "section": "9.4 Packages for descriptive statistics",
    "text": "9.4 Packages for descriptive statistics\nWe have covered in Section 8.3 several general packages for data exploration which includes descriptive statistics. In this, section we going to explore two useful packages for descriptive statistics: summarytools and gtsummary. It is to be noted that there are several other R packages available for descriptive statistics, to name a few: psych, pastecs, and Hmisc.\nsummarytools\nsummarytools provides an easy-to-use function for descriptive statistics. For numerical variables, we can use descr(). We going to use the iris dataset to demonstrate the capabilities of summarytools. More details on the iris dataset can be read by typing ?iris in the\n\n# Load packages\nlibrary(summarytools)\n\n# Descriptive statistics for numerical data\ndescr(iris %&gt;% select(-Species))\n\nDescriptive Statistics  \niris  \nN: 150  \n\n                    Petal.Length   Petal.Width   Sepal.Length   Sepal.Width\n----------------- -------------- ------------- -------------- -------------\n             Mean           3.76          1.20           5.84          3.06\n          Std.Dev           1.77          0.76           0.83          0.44\n              Min           1.00          0.10           4.30          2.00\n               Q1           1.60          0.30           5.10          2.80\n           Median           4.35          1.30           5.80          3.00\n               Q3           5.10          1.80           6.40          3.30\n              Max           6.90          2.50           7.90          4.40\n              MAD           1.85          1.04           1.04          0.44\n              IQR           3.50          1.50           1.30          0.50\n               CV           0.47          0.64           0.14          0.14\n         Skewness          -0.27         -0.10           0.31          0.31\n      SE.Skewness           0.20          0.20           0.20          0.20\n         Kurtosis          -1.42         -1.36          -0.61          0.14\n          N.Valid         150.00        150.00         150.00        150.00\n        Pct.Valid         100.00        100.00         100.00        100.00\n\n\nWe can see that descr() returns several familiar measures for descriptive statistics such as mean, standard deviation (Std.Dev), minimum value, maximum value, median, first and third quartile. There are several other measures that we have not covered previously:\n\nMAD:\n\nDescribes the median of the absolute deviations of the data points from the median of the dataset.\nA small MAD indicates that most data points are close to the median, reflecting low variability and a large MAD suggests greater spread, meaning data points are more dispersed from the median.\nUnlike the variance and standard deviation is less sensitive to outliers, thus, making it more suitable for a skewed distribution.\n\nCV:\n\nCV or coefficient of variation is a relative measure of variability. It reflects the ratio of standard deviation (\\(s^{2}\\)) to the mean (\\(\\overline{x}\\)).\n\\[\n\\begin{align*}\nCV = \\frac{s^{2}}{\\overline{x}}\\\\\n0.47 =\\frac{1.77}{3.76}\n\\end{align*}\n\\]\nLow CV indicates less variability relative to the mean, suggesting consistency and uniformity in the dataset, while high CV indicates greater variability relative to the mean, which could imply inconsistency or a wide spread of data values.\nThere is no exact value of CV that we can rate the variability of the data, however, as a rule of thumb:\n\nCV &lt; 0.3 indicates a low variability and spread which can be considered as good.\nCV &gt; 0.3 indicates a high variability, thus, requiring us to inspect the data distribution further.\n\nWe can recall the CV of each of the numeric variables in the iris dataset.\n\ndescr(iris %&gt;% select(-Species),\n      stats = c(\"CV\", \"mean\", \"sd\"),\n      transpose = TRUE,\n      headings = FALSE)\n\n\n                       CV   Mean   Std.Dev\n------------------ ------ ------ ---------\n      Petal.Length   0.47   3.76      1.77\n       Petal.Width   0.64   1.20      0.76\n      Sepal.Length   0.14   5.84      0.83\n       Sepal.Width   0.14   3.06      0.44\n\n\nSubsequently, we can reflect the values of CV for each numeric variable in the iris dataset to their respective histogram.\n\niris %&gt;% \n  pivot_longer(1:4, names_to = \"variable\", values_to = \"values\") %&gt;% \n  ggplot(aes(x = values, fill = variable)) +\n  geom_histogram(alpha = 0.6, bins = 30) +\n  facet_grid(rows = vars(variable)) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nSkewness and SE.skewness\n\nSkewness describes the symmetry of the distribution of the data.\nAs a rule of thumb (Hair et al. 2022):\n\nSkewness \\(\\approx\\) 0: indicates the data is nearly symmetry.\n-1 &lt; Skewness &lt; 1: indicates a low skewness, which is excellent.\n-2 &lt; Skewness &lt; 2: indicates a moderate skewness, which is acceptable.\nSkewness &gt; 2 or Skewness &lt; -2: indicates a substantial skewness, in which data distribution is not normal.\n\nA positive value indicates the tail of the histogram is longer on the right, indicating more extreme high values, while the negative skewness indicates the tail of the histogram is longer on the left, indicating more extreme low values.\nComparing skewness to the standard error (SE) of the skewness helps decide whether the asymmetry is meaningful or due to random variation in the sample.\n\\[\n\\frac{Skewness}{SE.skewness} &gt; \\pm 1.96 = \\text{significant skewness}\n\\]\n\nKurtosis\n\nKurtosis is a statistical measure that evaluates the extent to which data points cluster in the tails or the peak of the distribution compared to a normal distribution.\nIn simpler words, kurtosis indicates whether the data distribution is too “thin” or “broad” compared to a normal distribution.\nKurtosis and skewness are typically interpreted together to assess the shape of a data distribution. When both values are close to 0, it suggests that the data approximates a normal distribution.\nAs a rule of thumb (Hair et al. 2022):\n\nKurtosis \\(\\approx\\) 0: data approximate a normal distribution.\nKurtosis &gt; 2: data has a peaked distribution.\nKurtosis &lt; -2: data has a flat distribution.\n\n\nN.Valid: number of observations that is not a missing value.\nPct.Valid: percentage of the observations that is not a missing value.\n\nAdditionally, we can get descriptive statistics based on certain variables. For example, we can get the descriptive statistics of the numerical variables in the iris dataset according to the species.\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  descr(Petal.Length)\n\nDescriptive Statistics  \nPetal.Length by Species  \nData Frame: iris  \nN: 50  \n\n                    setosa   versicolor   virginica\n----------------- -------- ------------ -----------\n             Mean     1.46         4.26        5.55\n          Std.Dev     0.17         0.47        0.55\n              Min     1.00         3.00        4.50\n               Q1     1.40         4.00        5.10\n           Median     1.50         4.35        5.55\n               Q3     1.60         4.60        5.90\n              Max     1.90         5.10        6.90\n              MAD     0.15         0.52        0.67\n              IQR     0.18         0.60        0.78\n               CV     0.12         0.11        0.10\n         Skewness     0.10        -0.57        0.52\n      SE.Skewness     0.34         0.34        0.34\n         Kurtosis     0.65        -0.19       -0.37\n          N.Valid    50.00        50.00       50.00\n        Pct.Valid   100.00       100.00      100.00\n\n\nFor the descriptive statistics for the categorical variables, we can use the freq() function which calculates count and percentage.\n\n# Descriptive statistics for categorical data\nfreq(iris$Species)\n\nFrequencies  \niris$Species  \nType: Factor  \n\n                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n---------------- ------ --------- -------------- --------- --------------\n          setosa     50     33.33          33.33     33.33          33.33\n      versicolor     50     33.33          66.67     33.33          66.67\n       virginica     50     33.33         100.00     33.33         100.00\n            &lt;NA&gt;      0                               0.00         100.00\n           Total    150    100.00         100.00    100.00         100.00\n\n\nLastly, we can do a cross-tabulation between two categorical variables. To demonstrate this function, we need to create an additional categorical variable for the iris dataset.\n\niris %&gt;% \n  mutate(Sepal_Length_Cat = ifelse(Sepal.Length &gt; 5, \"large\", \"small\")) %&gt;% \n  select(Sepal_Length_Cat, Species) %&gt;% \n  ctable()\n\nError : Can't find NULL\n\n\nCross-Tabulation, Row Proportions  \n\"large\" * \"small\"  \nData Frame: iris  \n\n--------- --------- ------------ ------------ ------------ --------------\n            \"small\"       setosa   versicolor    virginica          Total\n  \"large\"                                                                \n    large             22 (18.6%)   47 (39.8%)   49 (41.5%)   118 (100.0%)\n    small             28 (87.5%)    3 ( 9.4%)    1 ( 3.1%)    32 (100.0%)\n    Total             50 (33.3%)   50 (33.3%)   50 (33.3%)   150 (100.0%)\n--------- --------- ------------ ------------ ------------ --------------\n\n\nThe first variable will appear as row names and the second variable will appear as column names. In our code, Sepal_Length_Cat will appear as a row, and Species will appear as a column.\ngtsummary\nThe gtsummary package provides an easy way to create a publication-ready summary table. We going to use the iris dataset to demonstrate the capabilities of gtsummary.\n\n# Load packages\nlibrary(gtsummary)\n\n# Create a summary table\ntbl_summary(data = iris)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 150\n1\n\n\n\n\nSepal.Length\n5.80 (5.10, 6.40)\n\n\nSepal.Width\n3.00 (2.80, 3.30)\n\n\nPetal.Length\n4.35 (1.60, 5.10)\n\n\nPetal.Width\n1.30 (0.30, 1.80)\n\n\nSpecies\n\n\n\n\n    setosa\n50 (33%)\n\n\n    versicolor\n50 (33%)\n\n\n    virginica\n50 (33%)\n\n\n\n1\nMedian (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\nWe can further adjust this summary table. For example, we want to:\n\nChange the median (Q1, Q3) to the mean and standard deviation.\nChange all values to 1 decimal point.\n\n\ntbl_summary(\n  data = iris,\n  statistic = all_continuous() ~ \"{mean} ({sd})\", # Use mean and standard deviation\n  digits = list(all_continuous() ~ 1, # numerical variables to 1 decimal place\n                all_categorical() ~ 1) # categorical variable to 1 decimal place\n)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 150\n1\n\n\n\n\nSepal.Length\n5.8 (0.8)\n\n\nSepal.Width\n3.1 (0.4)\n\n\nPetal.Length\n3.8 (1.8)\n\n\nPetal.Width\n1.2 (0.8)\n\n\nSpecies\n\n\n\n\n    setosa\n50.0 (33.3%)\n\n\n    versicolor\n50.0 (33.3%)\n\n\n    virginica\n50.0 (33.3%)\n\n\n\n1\nMean (SD); n (%)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html#chapter-summary",
    "href": "descriptive-stat.html#chapter-summary",
    "title": "9  Descriptive statistics",
    "section": "9.5 Chapter summary",
    "text": "9.5 Chapter summary\nIn this chapter, we have covered the basic descriptive statistics, which is consist of:\n\nMeasures of central tendency.\nMeasures of variability.\n\nWe have learned how to interpret and implement each measure in R. Additionally, we have covered two useful R packages for descriptive statistics:\n\nsummarytools\ngtsummary\n\nFor more information on summarytools (Comtois 2022), refer to its official documentation. Similarly, detailed guidance on gtsummary (Sjoberg et al. 2021) is available on its documentation page.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive-stat.html#revision",
    "href": "descriptive-stat.html#revision",
    "title": "9  Descriptive statistics",
    "section": "9.6 Revision",
    "text": "9.6 Revision\n\nExplain measures of central tendency.\nExplain measures of variability.\nWhat is the relationship of outliers with mean, median, and mode?\nExplain the differences between sample variance and population variance.\nRead about airquality data by typing ?airquality in the Console. Next, using the data:\n\ndata(\"airquality\")\n\n\nFind the median for each numerical variable in the dataset.\nUsing summarytools, get descriptive statistics for all variables.\nCreate a summary table using the gtsummary package.\n\nRead about CO2 data by typing ?CO2 in the Console. Next, using the data:\n\ndata(\"CO2\")\n\n\nGet descriptive statistics for uptake stratified by Type.\nCreate cross-tabulation between Type and Treatment with the former as a row and the latter as a column.\n\n\n\n\n\n\nComtois, Dominic. 2022. Summarytools: Tools to Quickly and Neatly Summarize Data. https://CRAN.R-project.org/package=summarytools.\n\n\nHair, Joseph F., G. Tomas M. Hult, Christian M. Ringle, and Marko Sarstedt. 2022. A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM). SAGE Publications, Inc.\n\n\nHyndman, Rob J., and Yanan Fan. 1996. “Sample Quantiles in Statistical Packages.” The American Statistician 50 (4): 361–65. https://doi.org/10.1080/00031305.1996.10473566.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  A way forward",
    "section": "",
    "text": "This book has provided a strong foundation for mastering R in the context of data analysis. However, R is an expansive tool with applications in numerous fields, and there are many avenues for further exploration once you have completed this book.\nGenerally, the field of data analysis is diverse and continually evolving. Here are three major areas that I can suggest to consider as part of your journey forward:\nGenerally, the field of data analysis .\n\nStatistical Analysis and Modeling\nStatistical analysis forms the backbone of data-driven decision-making. It involves descriptive techniques to summarize data and inferential methods to draw conclusions and test hypotheses. With R, you can perform regression analysis, ANOVA, time series forecasting, and multivariate analysis, among others.\nMachine Learning\nMachine learning builds on statistical methods to create algorithms capable of learning from data. R offers robust tools for machine learning, including packages like caret, tidymodels, and mlr3. These tools support tasks such as classification, clustering, and predictive modeling. R’s integration with deep learning libraries, like tensorflow, keras, and torch, also enables work in neural networks and reinforcement learning.\nData visualisation and communication\nEffective communication of results is key in data analysis. R’s visualization capabilities, through tools like ggplot2, plotly, and shiny, allow analysts to create dynamic, interactive, and publication-ready visualizations. Mastering these tools helps ensure that insights are clearly conveyed to both technical and non-technical audiences.\n\nWrapping up, mastering R isn’t just about knowing the basics—it’s your ticket to exploring a ton of exciting opportunities in data analysis and beyond. Whether you’re diving into statistical analysis to uncover trends or exploring the cutting-edge world of machine learning, R has got you covered. Plus, its powerful tools for creating stunning visualizations and tackling big data make it a must-have for any data enthusiast.\nAs you build on the foundation you’ve gained, think about where you want to specialize. Maybe it’s predictive modeling, visual storytelling through data, or even solving big, messy real-world problems. The possibilities are endless—so keep learning, experimenting, and pushing boundaries.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>A way forward</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Comtois, Dominic. 2022. Summarytools: Tools to Quickly and Neatly\nSummarize Data. https://CRAN.R-project.org/package=summarytools.\n\n\nCui, Boxuan. 2024. DataExplorer: Automate Data Exploration and\nTreatment. http://boxuancui.github.io/DataExplorer/.\n\n\nHair, Joseph F., G. Tomas M. Hult, Christian M. Ringle, and Marko\nSarstedt. 2022. A Primer on Partial Least Squares Structural\nEquation Modeling (PLS-SEM). SAGE Publications, Inc.\n\n\nHyndman, Rob J., and Yanan Fan. 1996. “Sample Quantiles in\nStatistical Packages.” The American Statistician 50 (4):\n361–65. https://doi.org/10.1080/00031305.1996.10473566.\n\n\nKowarik, Alexander, and Matthias Templ. 2016. “Imputation with the\nr Package VIM.” Journal of Statistical Software 74 (7):\n1–16. https://doi.org/10.18637/jss.v074.i07.\n\n\nRyu, Choonghyun. 2024. Dlookr: Tools for Data Diagnosis,\nExploration, Transformation. https://CRAN.R-project.org/package=dlookr.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery,\nand Joseph Larmarange. 2021. “Reproducible Summary Tables with the\nGtsummary Package.” The R Journal 13:\n570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nTierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data\nPrinciples to Facilitate Missing Data Exploration, Visualization and\nAssessment of Imputations.” Journal of Statistical\nSoftware 105 (7): 1–31. https://doi.org/10.18637/jss.v105.i07.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2024. Skimr: Compact and Flexible\nSummaries of Data. https://docs.ropensci.org/skimr/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. O’Reilly Media, Inc.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly Media, Inc.",
    "crumbs": [
      "References"
    ]
  }
]